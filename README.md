# Two birds with one stone

### Add a link to Kathryn's McLean's article

One of our core values at integrate.ai is to Love People. We mean this in every sense, as we shared in ([this post](https://medium.com/@integrate.ai/love-people-our-1-company-value-d62ab33f44ba)), but one that is pertinent to our machine learning team is to build models ethically, considering how their output may impact people's lives.

We work with large enterprises and apply machine learning to consumer behavioural data to help our clients increase the overall lifetime value of their customers, i.e., to guide them to the next best action or product to create stronger relationships with the business. There are different attributes and behaviours that we could consider when building our models and some of them are socially sensitive: for many use cases, we don't want things like gender, ethnicity, or marital status to influence our predictions. 

As much as we Love People, we also love cake (did anybody say red velvet?). And we really love having our cake and eating it too. When it comes to managing potential sources of bias in models, Variational Fair Autoencoders (VFAE) allow us to do just that. (KH COMMENT -- can you put a link here back to the original paper?) 

Briefly, autoencoders are machine learning models that reproduce the input data used to train an algorithm. This is in contrast to typical models, in which the target variable is different from the input. (KH COMMENT - explain what a target variable is. Also, maybe it would be a good idea to define "encoding" and "decoding" at this spot?) Autoencoders allow us to transform a dataset into a new mathematical representation, or latent space, that has a different representation of the original data. Why would we do this? One powerful application relates to helping an algorithm work better on real-world data: using a VAE, we can inject noise into the input, and while trying to predict the “clean” dataset, we will obtain a model that is very flexible to more messy, real-world data. (KH COMMENT - can you give one more example of why we'd use VAEs in practice before we shift to VFAEs? These are great!) 

The word variational refers to Bayesian techniques to approximate the posterior, i.e. the probability distribution of the output, the model is trying to predict. (KH COMMENT - add a few more sentences about this) 

Most interesting, and novel, is the fair component. In VFAEs, we explicitly give information about the properties of the data we do not want influencing our models. The latent version of a dataset generated by this model contains much of the valuable details of the original dataset, but without any influence of the sensitive information. We can then train models on this latent representation, and obtain good predictions that are not biased or influenced by sensitive properties of the people included in the dataset.

In our models we will use neural networks to transform the data through the encoding and decoding pieces. 

Let's explore how this might work using the Stanford Open Policing Project - Ohio ([link](https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-ohio)), which aggregates anonymous records of police arrest, both pedestrian and traffic, in the state of Ohio. This dataset includes sensitive information, such as the race of the people involved, which is something we definitely do not want to influence our model's predictions. We will use whether the person was arrested as the target variable.

(As a side note, we believe a use case like predicting if someone should be arrested shouldn't be fully automated by an algorithm; this kind of sensitive analysis requires human judgment (at least for the foreseeable future)).

Given the sensitivity of the task, we choose to optimize for precision in correctly classifying those who should be arrested. This would come at the expense of missing people who do, but we are OK with that. (KH COMMENT - say more here about the tradeoff between precision and recall and why you have to make a choice...) 

This dataset is highly imbalanced, with only about 0.6% of cases being arrests. Furthermore, it does not include many features (26 overall, with 16 being either of no use or redundant). Another challenge is that once the data is processed, which will be described below, the number of columns significantly expands due to categorical features being transformed into dummy variables. This results in a very sparse dataframe.

The first thing to do is to clean up the features. The following table lists all the features in the raw dataset, whether the feature will be kept, and reasons for doing so:

Feature | Keeping (Yes/No) | Reason
--- | :---: | ---
id | No | Irrelevant information
state | No | All are in Ohio
stop_date | Yes | The month of year might be an interesting feature (i.e. there might be some seasonal effect)
stop_time | Yes | The time of day is interesting because, for example, stops at night are more likely to result in arrests, as compared with stops during the day.
location_raw | Yes | There may be a dependence of location on odds of being arrested.
county_name | No | This feature is very similar to ‘locations_raw’
county_fips | No | This feature is very similar to ‘locations_raw’
fine_grained_location | No | This feature is very similar to ‘locations_raw’, but is too fine grained (containing 69204 unique values).
police_department | No | Only contains null values
driver_gender | Yes | An important feature
driver_age_raw | No | Only contains null values
driver_age | No | Only contains null values
driver_race_raw | No | Contains the same information as ‘driver_race’, but with some values unknown
driver_race | Yes | Will act as the sensitive feature we attempt to remove using VFAE
violations_raw | No | This is an extended version of ‘violations’, with too much detail
violations | Yes | An important feature, containing a distilled version of ‘violations_raw’
search_conducted | Yes | An important feature
search_type_raw | No | Contains mostly null values
search_type | No | Contains mostly null values
contraband_found | Yes | An important feature
stop_outcome | No | Very closely correlated with ‘is_arrested’
is_arrested | Yes | Our target variable
lat | No | Is essentially a very fine grained version of ‘location_raw’
lon | No | Is essentially a very fine grained version of ‘location_raw’
officer_id | No | There are too many unique values.
drug_related_stop | Yes | An important feature

### Let's clean and prepare the data

(KH COMMENT - I find the cleaning section to be a bit long and detailed - not sure you need to recount every step. That said, I do like what you noticed about the violations column...what would you remove to shorten this by 50% at least?) 

We take the time of day and convert it to an hour integer. 
We take the date and convert it to a month integer. 

To avoid including too many locations in the ‘location_raw’ feature, thereby significantly increasing the sparseness of the final dataframe, we keep only the most frequent 20 locations, which account for nearly 90% of all records. The remaining values are set to null, and will be imputed as described further down.

The violations column is an interesting one. It includes lists of strings, each denoting the violations (allegedly) performed by the person. For model simplicity we only want to focus on one, and we want to make sure it is the one that is most severe. Overall there are 12 unique violations included in the dataset, ordered here with #1 being the worst (which is a subjective definition):

1. dui
2. speeding
3. stop sign/light
4. license
5. cell phone
6. paperwork
7. registration/plates
8. safe movement
9. seat belt    
10. equipment
11. lights
12. truck
13. other
14. other (non-mapped)

We then replace the violations column with an integer column representing the worst violation attributed to the person. 

We take the remaining columns, which all contain categorical data, and replace null values with values taken from a probability distribution of the other values in the column. 

Finally, we then create dummy versions of every column.

The result is a data frame with 82 columns, which is now ready to be analyzed using a machine learning algorithm.

The classification model we will use will be Random Forest. 

Once we perform a pass on the dataset, we get the following results

[[[Precision and recall of the model]]]

However, it is upon a deeper look that we see an issue. Consider the distribution of the races of the people in the dataset

[[[Table which lists the distribution of race in the raw dataset, compared with the model’s prediction of arrests versus non-arrests]]]

We can see that after modelling, the fraction of African Americans and Hispanics increases, whereas the fraction of White and Asian decreases. We get a similar result if we look at the discrimination parameter proposed by Zemel et al. (2013)

[[[Discrimination values]]]

Clearly this is problematic, and this is where VFAE steps in.

By transforming the dataset using a VFAE, which involves injecting information regarding the race of the drivers into the model, we obtain a dataset which contains most of the useful information, but removes the dependence on race. Let us see what happens when we retrain the model. 

[[[Precision and recall of the model]]]

We see that we still obtain comparable performance as before, however, this time the model does not pick up the drivers races, and hence does not result in bias with respect to race.

[[[Table which lists the distribution of race in the raw dataset, compared with the model’s prediction of arrests versus non-arrests]]]

[[[Discrimination values]]]

These sorts of techniques are very exciting for integrate.ai because they allow us to train great models while making sure we are not accounting for information we neither need nor want to impact our predictions.

There are, of course, limitations to these models. For one, we need to explicitly specify what features we consider sensitive. This may not always be obvious. Also, given that we transform the dataset into a latent space, which is essentially abstract, we will have difficulty explaining the reasons for our model results. For example, in the above analysis although we can be confident that race is not a factor in predicting whether a person should be arrested, we would have trouble saying what is an important factor. This is where techniques such as FairML (KH COMMENT - can you provide a link back to FairML paper?), for example, can help.

Loving people means doing the best we can to ensure every person affected by our model is treated fairly. Doing so is a complicated task, especially when dealing with complicated non-linear models, but one that we proudly undertake. Using VFAE is just one of the tools we will employ to achieve this goal. We are investing engineering effort to ensure our infrastructure is secure, are creating partnerships with the Vector Institute to explore technical solutions with leading researchers in the field, and are making deliberate choices about the projects we accept from clients so we can say with integrity that we are applying AI to make the world a better place. 
