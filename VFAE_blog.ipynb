{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.framework.python.ops import add_arg_scope, arg_scope, arg_scoped_arguments\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, fbeta_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "f_half_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/OH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the columns that will be used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workable_cols = ['stop_date','stop_time','location_raw','driver_gender','driver_race','driver_race_raw','violation',\n",
    "                 'search_conducted','contraband_found','is_arrested','drugs_related_stop']\n",
    "\n",
    "df_reduced = df[workable_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert time into hours and date into month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced['stop_hour'] = df_reduced['stop_time'].apply(lambda x: x.split(':')[0])\n",
    "df_reduced['stop_month'] = df_reduced['stop_date'].apply(lambda x: x.split('-')[1])\n",
    "df_reduced.drop('stop_time',axis=1,inplace=True)\n",
    "df_reduced.drop('stop_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the 20 most frequent locations, which account for ~90% of all the locations. The remainder is set to null, to be imputed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_reduced['location_raw'].value_counts()[:20].sum()/df_reduced.shape[0])\n",
    "keep_locations = list(df_reduced['location_raw'].value_counts().keys()[:20])\n",
    "df_reduced['location_raw'] = df_reduced['location_raw'].apply(lambda x: int(x) if x in keep_locations else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two columns for driver race. Impute the 'other' values in one, with the values from the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced['driver_race'].loc[df_reduced['driver_race']=='Other'] = df_reduced['driver_race_raw'].loc[df_reduced['driver_race']=='Other']\n",
    "#df_reduced['driver_race'] = df_reduced.apply(lambda x: x['driver_race_raw'] if x['driver_race'] == 'Other' else x['driver_race'],axis=1)\n",
    "df_reduced.drop('driver_race_raw',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a realtively small number of unique violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "violations_unique = []\n",
    "for l in df_reduced['violation'].dropna().unique():\n",
    "    violations_unique = violations_unique + l.lower().split(',')\n",
    "violations_unique = set(violations_unique)\n",
    "print(violations_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping between violations to integer values, which are ranked from worst to least\n",
    "### The worst one will be selected in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def violations_denoter(x):\n",
    "    \n",
    "    violations_severity = {\n",
    "        'dui':0,\n",
    "        'speeding':1,\n",
    "        'stop sign/light':2,\n",
    "        'license':3,\n",
    "        'cell phone':4,\n",
    "        'paperwork':5,\n",
    "        'registration/plates':6,\n",
    "        'safe movement':7,\n",
    "        'seat belt':8,    \n",
    "        'equipment':9,\n",
    "        'lights':10,\n",
    "        'truck':11,\n",
    "        'other':12,\n",
    "        'other (non-mapped)':13\n",
    "    }\n",
    "    \n",
    "    violations = []\n",
    "    for k,v in violations_severity.items():\n",
    "        if (k in x.lower()):\n",
    "            violations.append(v)\n",
    "            \n",
    "    return min(violations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced['violations_numbered'] = df_reduced['violation'].fillna('other (non-mapped)').apply(violations_denoter)\n",
    "df_reduced.drop('violation',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = list(df_reduced.columns)\n",
    "categorical.remove('is_arrested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_na_categorical(df,column):\n",
    "\n",
    "    prob = dict(df[column].value_counts()/len(df))\n",
    "    \n",
    "    keys = list(prob.keys())\n",
    "    \n",
    "    sum_prob = sum(prob.values())\n",
    "\n",
    "    for k,v in prob.items():\n",
    "        prob[k] = prob[k]/sum_prob\n",
    "\n",
    "    prob_list = list(prob.values())\n",
    "\n",
    "    to_fillin = np.random.choice(keys, len(df[column].loc[df[column].isnull()]), p = prob_list)\n",
    "    \n",
    "    df[column].loc[df[column].isnull()] = to_fillin\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in categorical:\n",
    "    df_reduced = replace_na_categorical(df_reduced,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(df_reduced,columns=categorical)\n",
    "dummy.to_csv('data/df_cleaned.csv',index=False)\n",
    "dummy_sampled = dummy.sample(frac=0.1,random_state=2018).reset_index(drop=True)\n",
    "dummy_sampled.to_csv('data/df_sampled_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set up the model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_HPS_search(train_X,train_Y,model,n_iter,score,cv,weights=None):\n",
    "    \n",
    "    model_selection, params = get_model_and_params(model)\n",
    "    \n",
    "    RS = RandomizedSearchCV(model_selection,param_distributions=params,n_iter=n_iter,scoring=score,cv=cv,error_score='0')\n",
    "    RS.fit(train_X,train_Y,sample_weight=weights)\n",
    "    print(\"Best params - \", RS.best_params_)\n",
    "    print(\"Highest %s = %s\"%(score,RS.best_score_))\n",
    "    \n",
    "    return RS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_and_params(model):\n",
    "    \n",
    "    model_selection = {\n",
    "        'RFC':RandomForestClassifier(),\n",
    "        'GBC':GradientBoostingClassifier(),\n",
    "        'LR':LogisticRegression()\n",
    "    }\n",
    "    \n",
    "    model_hyperparameters = {\n",
    "        'RFC':{\n",
    "            'n_estimators':range(1,101),\n",
    "            'max_depth':range(1,50),\n",
    "            'n_jobs':[-1],\n",
    "            'criterion':['gini','entropy'],\n",
    "            'class_weight':[None]\n",
    "        },\n",
    "        'GBC':{\n",
    "            'loss':['deviance','exponential'],\n",
    "            'learning_rate':10**np.linspace(-4,-1,10),\n",
    "            'n_estimators':range(50,150)\n",
    "        },\n",
    "        'LR':{\n",
    "            'C':10.**np.linspace(-3,3,20),\n",
    "            'tol':10**np.linspace(-5,-1,20),\n",
    "            'penalty':['l2'],\n",
    "            'class_weight':['balanced']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return model_selection[model], model_hyperparameters[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_clf(model,train_X,train_Y,params,weights=None):\n",
    "    \n",
    "    clf = get_model_and_params(model)[0]\n",
    "    \n",
    "    clf.set_params(**params)\n",
    "    clf.fit(train_X,train_Y,sample_weight=weights)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_error_and_discrimination(clf,test_X,test_Y,df_test,sensitive_features):\n",
    "\n",
    "    Y_predict = clf.predict(test_X)\n",
    "\n",
    "    print(\"F1 score = %s\" %f1_score(test_Y,Y_predict))\n",
    "    print(\"Precision score = %s\" %precision_score(test_Y,Y_predict))\n",
    "    print(\"Recall score = %s\" %recall_score(test_Y,Y_predict))\n",
    "    print(\"Accuracy score = %s\" %accuracy_score(test_Y,Y_predict))\n",
    "\n",
    "    print()\n",
    "\n",
    "    s = np.asarray(df_test[sensitive_features])\n",
    "    print(\"Discrimination_ratio = %s\" %(discrimination(np.expand_dims(Y_predict,1),s)))\n",
    "\n",
    "    print()\n",
    "\n",
    "    features = ['driver_race_Asian','driver_race_Black','driver_race_White','driver_race_Hispanic']\n",
    "    \n",
    "    bias = [\n",
    "    features,\n",
    "    list(df_test[features].mean()),\n",
    "    list(df_test[features].loc[Y_predict==1].mean()),\n",
    "    list(df_test[features].loc[Y_predict==0].mean())\n",
    "    ]\n",
    "        \n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrimination(y,s):\n",
    "    \n",
    "    P_a1_s1 = float(y[s==1].sum())/float(len(s[s==1]))\n",
    "    P_a1_s0 = float(y[s==0].sum())/float(len(s[s==0]))\n",
    "    \n",
    "    disc_ratio =  P_a1_s1/P_a1_s0\n",
    "    \n",
    "    return disc_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_df(df,f_balance):\n",
    "    \n",
    "    if (f_balance == -1):\n",
    "        return df\n",
    "    \n",
    "    cond = df['is_arrested']==True\n",
    "\n",
    "    df_balanced = pd.concat(\n",
    "        [df.loc[cond],\n",
    "         df.loc[~cond].sample(n=df.loc[cond].shape[0]*f_balance)],\n",
    "        axis=0\n",
    "    ).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_distribution(df_bias):\n",
    "    iai_colors = {\n",
    "        'blue': np.array([72, 196, 217]) / 255,\n",
    "        'ruby': np.array([240, 86, 60]) / 255,\n",
    "        'grey': np.array([49, 64, 73]) / 255,\n",
    "        'beige': np.array([241, 231, 220]) / 255\n",
    "    }\n",
    "\n",
    "    plt.bar(np.arange(df_bias.shape[0])-0.2,df_bias['N_group/N_total']*100,0.2,color=iai_colors['grey'])\n",
    "    plt.bar(np.arange(df_bias.shape[0]),df_bias['N_group/N_total|(predicted arrest)']*100,0.2,color=iai_colors['ruby'])\n",
    "    plt.bar(np.arange(df_bias.shape[0])+0.2,df_bias['N_group/N_total|(predicted not arrest)']*100,0.2,color=iai_colors['blue'])\n",
    "    plt.xticks(np.arange(df_bias.shape[0]),('Asian','Black','White','Hispanic'))\n",
    "    plt.ylabel('% of people in each group')\n",
    "    plt.ylim([0,100])\n",
    "    plt.legend(['Overall','Predicted arrested','Predicted not arrested'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/df_sampled_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feature = 'is_arrested'\n",
    "sensitive_features = ['driver_race_Black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray(df_train.drop([target_feature],axis=1))\n",
    "train_Y = np.asarray(df_train[target_feature]).astype(int)\n",
    "\n",
    "test_X = np.asarray(df_test.drop([target_feature],axis=1))\n",
    "test_Y = np.asarray(df_test[target_feature]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.21681723419\n",
      "Precision score = 0.555160142349\n",
      "Recall score = 0.134715025907\n",
      "Accuracy score = 0.993907449454\n",
      "\n",
      "Discrimination_ratio = 3.256294240669241\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a294a98d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'n_jobs': -1, 'n_estimators': 47, 'max_depth': 8, 'criterion': 'gini', 'class_weight': None}\n",
    "weights = np.ones(train_Y.shape[0])\n",
    "weights[train_Y==1] = 1.\n",
    "weights[train_Y==0] = 0.5\n",
    "RFC = run_clf('RFC',train_X,train_Y,params,weights=weights)\n",
    "bias = check_error_and_discrimination(RFC,test_X,test_Y,df_test,sensitive_features)\n",
    "df_bias = pd.DataFrame(bias).transpose()\n",
    "df_bias.columns = [\"Group\",\"N_group/N_total\",\"N_group/N_total|(predicted arrest)\",\"N_group/N_total|(predicted not arrest)\"]\n",
    "plot_distribution(df_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin setting up the VFAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights_biases(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=tf.sqrt(0.5 / float(shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activate(x,activation):\n",
    "    if (activation == 'relu'):\n",
    "        return tf.nn.relu(x)\n",
    "    elif (activation == 'sigmoid'):\n",
    "        return tf.nn.sigmoid(x)\n",
    "    elif (activation == 'tanh'):\n",
    "        return tf.nn.tanh(x)\n",
    "    elif (activation == 'softmax'):\n",
    "        return tf.nn.softmax(x)\n",
    "    elif (activation == 'linear'):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(list_arrays,batch_size,index_shuffled,b):\n",
    "    \n",
    "    return [x[index_shuffled[b*batch_size:(b+1)*batch_size]] for x in list_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(dims,N_epochs=1000,print_freq=100,batch_size=100,lr=1e-3,alpha=1.,beta=1.,D=500,gamma=1.):\n",
    "\n",
    "    params = {\n",
    "        'enc1':{\n",
    "            'in_dim':dims['x']+dims['s'],\n",
    "            'hid_dim':dims['enc1_hid'],\n",
    "            'out_dim':dims['z1'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'linear',\n",
    "                'log_sigma':'linear'\n",
    "            }\n",
    "        },   \n",
    "        'enc2':{\n",
    "            'in_dim':dims['z1']+1,\n",
    "            'hid_dim':dims['enc2_hid'],\n",
    "            'out_dim':dims['z2'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'linear',\n",
    "                'log_sigma':'linear'\n",
    "            }\n",
    "        },\n",
    "        'dec1':{\n",
    "            'in_dim':dims['z2']+1,\n",
    "            'hid_dim':dims['dec1_hid'],\n",
    "            'out_dim':dims['z1'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'linear',\n",
    "                'log_sigma':'linear'\n",
    "            }\n",
    "        },\n",
    "        'dec2':{\n",
    "            'in_dim':dims['z1']+dims['s'],\n",
    "            'hid_dim':dims['dec2_hid'],\n",
    "            'out_dim':dims['x']+dims['s'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'sigmoid',\n",
    "                'log_sigma':'sigmoid',\n",
    "                'bernoulli':'sigmoid'\n",
    "            }\n",
    "        },\n",
    "        'us':{\n",
    "            'in_dim':dims['z1'],\n",
    "            'hid_dim':dims['us_hid'],\n",
    "            'out_dim':dims['y_cat'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'out':'softmax'\n",
    "            }\n",
    "        },\n",
    "        'N_epochs':N_epochs,\n",
    "        'print_frequency':print_freq,\n",
    "        'batch_size':batch_size,\n",
    "        'lr':lr,\n",
    "        'alpha':alpha,\n",
    "        'beta':beta,\n",
    "        'D':D,\n",
    "        'gamma':gamma\n",
    "    }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights_biases(params):\n",
    "\n",
    "    weights = {\n",
    "        'enc1':{\n",
    "            'hid':gen_weights_biases([params['enc1']['in_dim'],params['enc1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc1']['hid_dim'],params['enc1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc1']['hid_dim'],params['enc1']['out_dim']])\n",
    "        },\n",
    "        'enc2':{\n",
    "            'hid':gen_weights_biases([params['enc2']['in_dim'],params['enc2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc2']['hid_dim'],params['enc2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc2']['hid_dim'],params['enc2']['out_dim']])\n",
    "        },\n",
    "        'dec1':{\n",
    "            'hid':gen_weights_biases([params['dec1']['in_dim'],params['dec1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec1']['hid_dim'],params['dec1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec1']['hid_dim'],params['dec1']['out_dim']])\n",
    "        },\n",
    "        'dec2':{\n",
    "            'hid':gen_weights_biases([params['dec2']['in_dim'],params['dec2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec2']['hid_dim'],params['dec2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec2']['hid_dim'],params['dec2']['out_dim']])\n",
    "        },\n",
    "        'us':{\n",
    "            'hid':gen_weights_biases([params['us']['in_dim'],params['us']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['us']['hid_dim'],params['us']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['us']['hid_dim'],params['us']['out_dim']])\n",
    "        }       \n",
    "    }\n",
    "\n",
    "    bias = {\n",
    "        'enc1':{\n",
    "            'hid':gen_weights_biases([params['enc1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc1']['out_dim']])\n",
    "        },\n",
    "        'enc2':{\n",
    "            'hid':gen_weights_biases([params['enc2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc2']['out_dim']])\n",
    "        },\n",
    "        'dec1':{\n",
    "            'hid':gen_weights_biases([params['dec1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec1']['out_dim']])\n",
    "        },\n",
    "        'dec2':{\n",
    "            'hid':gen_weights_biases([params['dec2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec2']['out_dim']])\n",
    "        },\n",
    "        'us':{\n",
    "            'hid':gen_weights_biases([params['us']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['us']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['us']['out_dim']])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x_in,weights,bias,activation,epsilon):\n",
    "    \n",
    "    hidden = activate(tf.matmul(x_in,weights['hid'])+bias['hid'],activation['hid'])\n",
    "\n",
    "    mu = activate(tf.matmul(hidden,weights['mu'])+bias['mu'],activation['mu'])\n",
    "\n",
    "    log_sigma = activate(tf.matmul(hidden,weights['log_sigma'])+bias['log_sigma'],activation['log_sigma'])\n",
    "\n",
    "    return mu + tf.exp(log_sigma / 2) * epsilon, mu, log_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KL(mu1,log_sigma_sq1,mu2=0.,log_sigma_sq2=0.):\n",
    "    return 0.5*tf.reduce_sum(log_sigma_sq2-log_sigma_sq1-1+(tf.exp(log_sigma_sq1)+tf.pow(mu1-mu2,2))/tf.exp(log_sigma_sq2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LH(x,mu,log_sigma):\n",
    "    return 0.5 * tf.reduce_sum(np.log(2 * np.pi) + log_sigma + tf.pow(x - mu,2) / tf.exp(log_sigma), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_MMD(x1,x2,params):\n",
    "    inner_difference = tf.reduce_mean(psi(x1,params),axis=0) - tf.reduce_mean(psi(x2,params),axis=0)\n",
    "    return tf.tensordot(inner_difference,inner_difference,axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def psi(x,params):\n",
    "    W = tf.Variable(tf.random_normal([params['enc1']['out_dim'],params['D']], \n",
    "                         stddev=tf.sqrt(0.5 / float(params['enc1']['out_dim'])),\n",
    "                         dtype=tf.float32))\n",
    "    b = tf.Variable(tf.random_uniform([params['D']],0,2*np.pi,dtype=tf.float32))\n",
    "    \n",
    "    return tf.pow(2./params['D'],0.5)*tf.cos(tf.pow(2./params['gamma'],0.5)*tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_VFAE(train_X,train_Y,train_s,test_X,test_Y,test_s,weights,bias,params,dims):\n",
    "\n",
    "    x = tf.placeholder(tf.float32,shape=[None,dims['x']],name='x')\n",
    "    s = tf.placeholder(tf.float32,shape=[None,dims['s']],name='s')\n",
    "    y = tf.placeholder(tf.float32,shape=[None,1],name='y')\n",
    "    \n",
    "    epsilon0 = tf.random_normal([params['enc1']['out_dim']], dtype=tf.float32, name='epsilon0')\n",
    "    z1_enc, z1_enc_mu, z1_enc_log_sigma = MLP(tf.concat([x,s],axis=1),weights['enc1'],bias['enc1'],params['enc1']['act'],epsilon0)\n",
    "    \n",
    "    epsilon1 = tf.random_normal([params['enc2']['out_dim']], dtype=tf.float32, name='epsilon1')\n",
    "    z2_enc, z2_enc_mu, z2_enc_log_sigma = MLP(tf.concat([z1_enc,y],axis=1),weights['enc2'],bias['enc2'],params['enc2']['act'],epsilon1)\n",
    "    \n",
    "    epsilon2 = tf.random_normal([params['dec1']['out_dim']], dtype=tf.float32, name='epsilon2')\n",
    "    z1_dec, z1_dec_mu, z1_dec_log_sigma = MLP(tf.concat([z2_enc,y],axis=1),weights['dec1'],bias['dec1'],params['dec1']['act'],epsilon2)\n",
    "    \n",
    "    epsilon3 = tf.zeros([params['dec2']['out_dim']], dtype=tf.float32, name='epsilon3')\n",
    "    x_out = MLP(tf.concat([z1_dec,s],axis=1),weights['dec2'],bias['dec2'],params['dec2']['act'],epsilon3)[0]\n",
    "\n",
    "    epsilon4 = tf.zeros([params['us']['out_dim']], dtype=tf.float32, name='epsilon4')\n",
    "    y_us = MLP(z1_enc,weights['us'],bias['us'],params['us']['act'],epsilon4)[0]\n",
    "    \n",
    "    KL_z1 = KL(z1_enc_mu,z1_enc_log_sigma,z1_dec_mu,z1_dec_log_sigma)\n",
    "    KL_z2 = KL(z2_enc_mu,z2_enc_log_sigma)\n",
    "    \n",
    "    LH_x = tf.reduce_sum(tf.concat([x,s],axis=1) * tf.log(1e-10+x_out) + (1 - tf.concat([x,s],axis=1)) * tf.log(1e-10+1 - x_out),axis=1)\n",
    "    \n",
    "    index = tf.range(tf.shape(y)[0])\n",
    "        \n",
    "    idx = tf.stack([index[:, tf.newaxis], tf.cast(y,tf.int32)], axis=-1)\n",
    "    \n",
    "    LH_y = tf.reduce_sum(tf.log(1e-10+tf.gather_nd(y_us, idx)),axis=1)\n",
    "\n",
    "    MMD_x1 = tf.boolean_mask(z1_enc,tf.tile(tf.cast(s,tf.bool),[1,tf.shape(z1_enc)[1]]))\n",
    "    MMD_x2 = tf.boolean_mask(z1_enc,tf.tile(tf.cast(1-s,tf.bool),[1,tf.shape(z1_enc)[1]]))\n",
    "    MMD = fast_MMD(tf.reshape(MMD_x1,[tf.cast(tf.shape(MMD_x1)[0]/tf.shape(z1_enc)[1],tf.int32),tf.shape(z1_enc)[1]]),\n",
    "                   tf.reshape(MMD_x2,[tf.cast(tf.shape(MMD_x2)[0]/tf.shape(z1_enc)[1],tf.int32),tf.shape(z1_enc)[1]]),\n",
    "                  params)\n",
    "\n",
    "    loss = -(-tf.reduce_mean(KL_z1)-tf.reduce_mean(KL_z2)+tf.reduce_mean(LH_x) - params['alpha']*tf.reduce_mean(LH_y) - params['beta']*MMD)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['lr'])\n",
    "\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    index_shuffled = np.arange(train_X.shape[0])\n",
    "    np.random.shuffle(index_shuffled)\n",
    "\n",
    "    N_batches = int(float(train_X.shape[0])/float(params['batch_size']))\n",
    "    \n",
    "    for i in range(params['N_epochs']):\n",
    "        \n",
    "        for b in range(N_batches):\n",
    "            \n",
    "            batch_X, batch_Y, batch_s = get_batch([train_X,train_Y,train_s],params['batch_size'],index_shuffled,b)\n",
    "\n",
    "            batch_dict = {x:batch_X,y:batch_Y,s:batch_s}\n",
    "            full_dict = {x:train_X,y:train_Y,s:train_s}\n",
    "\n",
    "            sess.run(train,feed_dict=batch_dict)\n",
    "\n",
    "        if (i % params['print_frequency'] == 0 or i == params['N_epochs']-1):\n",
    "\n",
    "            print(\"Epoch %s: batch loss = %s and global loss = %s\"%(i,\n",
    "                    sess.run(loss,feed_dict=batch_dict),\n",
    "                    sess.run(loss,feed_dict=full_dict)))\n",
    "            print(\"KL_z1 = %s, KL_z2 = %s, RL = %s, unsupervised posterior = %s, MMD = %s\" \\\n",
    "                  %sess.run((tf.reduce_mean(KL_z1),\n",
    "                             tf.reduce_mean(KL_z2),\n",
    "                             tf.reduce_mean(LH_x),\n",
    "                             tf.reduce_mean(LH_y),\n",
    "                             params['beta']*MMD\n",
    "                            ),\n",
    "                            feed_dict=batch_dict))\n",
    "            \n",
    "    test_dict = {x:test_X,y:test_Y,s:test_s}\n",
    "    return sess.run([x_out,z1_enc,loss,tf.reduce_mean(LH_x)],feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into a training and cross-validation sets, to determine the optimal hyperparameters for the VFAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_X_Y_s(df,target_feature,sensitive_features):\n",
    "    \n",
    "    X = np.asarray(df.drop([target_feature]+sensitive_features,axis=1))\n",
    "    Y = np.expand_dims(np.asarray(df[target_feature]).astype(int),1)\n",
    "    s = np.asarray(df[sensitive_features]).astype(int)\n",
    "\n",
    "    return X, Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_VFAE_train, df_VFAE_CV = train_test_split(df,test_size=0.5)\n",
    "df_VFAE_CV, df_VFAE_test = train_test_split(df_VFAE_CV,test_size=0.75)\n",
    "\n",
    "df_VFAE_train = df_VFAE_train.reset_index(drop=True)\n",
    "df_VFAE_CV = df_VFAE_CV.reset_index(drop=True)\n",
    "df_VFAE_test = df_VFAE_test.reset_index(drop=True)\n",
    "\n",
    "VFAE_train_X, VFAE_train_Y, VFAE_train_s = obtain_X_Y_s(df_VFAE_train,target_feature,sensitive_features)\n",
    "VFAE_CV_X, VFAE_CV_Y, VFAE_CV_s = obtain_X_Y_s(df_VFAE_CV,target_feature,sensitive_features)\n",
    "VFAE_test_X, VFAE_test_Y, VFAE_test_s = obtain_X_Y_s(df_VFAE_test,target_feature,sensitive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a hyperparamter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 0.1, D = 50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yevgenik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: batch loss = 15.063 and global loss = 15.0794\n",
      "KL_z1 = 1.40626, KL_z2 = 0.270414, RL = -18.605, unsupervised posterior = -22.8647, MMD = 0.105036\n",
      "Epoch 5: batch loss = 13.9147 and global loss = 14.1827\n",
      "KL_z1 = 0.161108, KL_z2 = 0.0160111, RL = -16.0559, unsupervised posterior = -22.8647, MMD = 0.00327136\n",
      "The CV global loss and reconstruction loss = 13.3014 and -15.4828\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = 15.5433 and global loss = 18.3853\n",
      "KL_z1 = 1.06085, KL_z2 = 0.503575, RL = -16.7037, unsupervised posterior = -22.7956, MMD = 0.0967684\n",
      "Epoch 5: batch loss = 13.6276 and global loss = 13.5029\n",
      "KL_z1 = 0.304481, KL_z2 = 0.0489052, RL = -15.6321, unsupervised posterior = -22.7956, MMD = 0.00715224\n",
      "The CV global loss and reconstruction loss = 13.2092 and -15.4441\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 16.0062 and global loss = 16.1289\n",
      "KL_z1 = 1.56535, KL_z2 = 0.221942, RL = -17.7027, unsupervised posterior = -22.7496, MMD = 0.0687485\n",
      "Epoch 5: batch loss = 13.5961 and global loss = 13.8846\n",
      "KL_z1 = 0.0909506, KL_z2 = 0.0182693, RL = -15.9498, unsupervised posterior = -22.7496, MMD = 0.00241577\n",
      "The CV global loss and reconstruction loss = 13.3376 and -15.5042\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = 15.8197 and global loss = 16.9034\n",
      "KL_z1 = 1.14447, KL_z2 = 0.41555, RL = -18.8111, unsupervised posterior = -22.8186, MMD = 0.771622\n",
      "Epoch 5: batch loss = 13.2518 and global loss = 14.4048\n",
      "KL_z1 = 0.144867, KL_z2 = 0.0692886, RL = -15.3148, unsupervised posterior = -22.8186, MMD = 0.103992\n",
      "The CV global loss and reconstruction loss = 13.3747 and -15.4999\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = 18.9215 and global loss = 16.2744\n",
      "KL_z1 = 0.818076, KL_z2 = 0.303423, RL = -17.0322, unsupervised posterior = -22.9568, MMD = 0.777839\n",
      "Epoch 5: batch loss = 13.3507 and global loss = 13.505\n",
      "KL_z1 = 0.0861906, KL_z2 = 0.015549, RL = -15.4314, unsupervised posterior = -22.9568, MMD = 0.0347206\n",
      "The CV global loss and reconstruction loss = 13.3487 and -15.5456\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = 16.0371 and global loss = 16.2954\n",
      "KL_z1 = 0.762379, KL_z2 = 0.459732, RL = -16.6858, unsupervised posterior = -22.8644, MMD = 0.70736\n",
      "Epoch 5: batch loss = 13.9747 and global loss = 13.8817\n",
      "KL_z1 = 0.156318, KL_z2 = 0.044817, RL = -15.8498, unsupervised posterior = -22.8647, MMD = 0.0162027\n",
      "The CV global loss and reconstruction loss = 13.6841 and -15.4547\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 16.7003 and global loss = 18.6666\n",
      "KL_z1 = 3.1725, KL_z2 = 0.176815, RL = -16.5019, unsupervised posterior = -22.8877, MMD = 0.942073\n",
      "Epoch 5: batch loss = 13.4073 and global loss = 13.5196\n",
      "KL_z1 = 0.472515, KL_z2 = 0.0481408, RL = -15.4768, unsupervised posterior = -22.8877, MMD = 0.227622\n",
      "The CV global loss and reconstruction loss = 13.7339 and -15.437\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 19.201 and global loss = 19.0005\n",
      "KL_z1 = 1.90701, KL_z2 = 0.161586, RL = -16.0645, unsupervised posterior = -22.8877, MMD = 1.55823\n",
      "Epoch 5: batch loss = 13.6118 and global loss = 13.531\n",
      "KL_z1 = 0.709528, KL_z2 = 0.0280701, RL = -15.4786, unsupervised posterior = -22.8877, MMD = 0.31276\n",
      "The CV global loss and reconstruction loss = 13.6058 and -15.6275\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 1000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 19.5203 and global loss = 20.0019\n",
      "KL_z1 = 3.0614, KL_z2 = 0.140296, RL = -16.2743, unsupervised posterior = -22.9567, MMD = 2.34454\n",
      "Epoch 5: batch loss = 13.6841 and global loss = 13.7268\n",
      "KL_z1 = 0.29328, KL_z2 = 0.041987, RL = -15.5026, unsupervised posterior = -22.9568, MMD = 0.128302\n",
      "The CV global loss and reconstruction loss = 13.4203 and -15.4657\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = 47.7615 and global loss = 46.9307\n",
      "KL_z1 = 2.03199, KL_z2 = 1.35422, RL = -46.4802, unsupervised posterior = -0.501577, MMD = 0.179675\n",
      "Epoch 5: batch loss = 16.1492 and global loss = 22.4499\n",
      "KL_z1 = 1.2657, KL_z2 = 0.456836, RL = -16.9279, unsupervised posterior = -21.3861, MMD = 0.0951821\n",
      "The CV global loss and reconstruction loss = 17.9992 and -18.2227\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = 52.5131 and global loss = 51.8223\n",
      "KL_z1 = 0.939764, KL_z2 = 0.522686, RL = -48.4905, unsupervised posterior = -0.579617, MMD = 0.185234\n",
      "Epoch 5: batch loss = 16.9725 and global loss = 17.0721\n",
      "KL_z1 = 0.931852, KL_z2 = 0.945393, RL = -21.0525, unsupervised posterior = -22.861, MMD = 0.157322\n",
      "The CV global loss and reconstruction loss = 15.7052 and -16.9529\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 51.3381 and global loss = 53.9366\n",
      "KL_z1 = 0.734194, KL_z2 = 0.544952, RL = -49.2049, unsupervised posterior = -1.08018, MMD = 0.200961\n",
      "Epoch 5: batch loss = 18.1701 and global loss = 16.7459\n",
      "KL_z1 = 1.30954, KL_z2 = 0.493356, RL = -18.2175, unsupervised posterior = -22.8927, MMD = 0.13501\n",
      "The CV global loss and reconstruction loss = 15.7096 and -17.225\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = 50.9121 and global loss = 47.6928\n",
      "KL_z1 = 2.13523, KL_z2 = 2.63801, RL = -44.6269, unsupervised posterior = -1.03655, MMD = 1.79668\n",
      "Epoch 5: batch loss = 16.7763 and global loss = 21.4401\n",
      "KL_z1 = 1.34453, KL_z2 = 1.01069, RL = -17.5003, unsupervised posterior = -20.8007, MMD = 1.25036\n",
      "The CV global loss and reconstruction loss = 15.5974 and -16.3991\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = 54.1226 and global loss = 51.8417\n",
      "KL_z1 = 0.976091, KL_z2 = 1.54338, RL = -45.0721, unsupervised posterior = -0.644951, MMD = 1.48596\n",
      "Epoch 5: batch loss = 18.8645 and global loss = 17.8015\n",
      "KL_z1 = 1.70584, KL_z2 = 0.99293, RL = -16.5923, unsupervised posterior = -22.8549, MMD = 1.28365\n",
      "The CV global loss and reconstruction loss = 16.2895 and -16.5529\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = 48.0809 and global loss = 50.4034\n",
      "KL_z1 = 0.726911, KL_z2 = 0.942075, RL = -49.2676, unsupervised posterior = -0.48541, MMD = 1.69885\n",
      "Epoch 5: batch loss = 16.957 and global loss = 17.0257\n",
      "KL_z1 = 1.8033, KL_z2 = 0.701161, RL = -21.227, unsupervised posterior = -21.7434, MMD = 1.32604\n",
      "The CV global loss and reconstruction loss = 16.7767 and -16.2591\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 67.7255 and global loss = 70.2442\n",
      "KL_z1 = 1.3359, KL_z2 = 1.4823, RL = -48.6203, unsupervised posterior = -1.7408, MMD = 13.9175\n",
      "Epoch 5: batch loss = 24.837 and global loss = 24.4026\n",
      "KL_z1 = 2.34948, KL_z2 = 0.420613, RL = -17.2913, unsupervised posterior = -22.8742, MMD = 3.59185\n",
      "The CV global loss and reconstruction loss = 18.2692 and -16.4094\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 71.5273 and global loss = 64.1083\n",
      "KL_z1 = 1.24667, KL_z2 = 0.887885, RL = -47.1515, unsupervised posterior = -1.05269, MMD = 16.7378\n",
      "Epoch 5: batch loss = 24.2535 and global loss = 29.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_z1 = 2.29597, KL_z2 = 0.322634, RL = -18.2406, unsupervised posterior = -21.4258, MMD = 6.29572\n",
      "The CV global loss and reconstruction loss = 19.3614 and -16.9772\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, batch size = 10000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 66.3103 and global loss = 67.0616\n",
      "KL_z1 = 1.27234, KL_z2 = 2.16611, RL = -45.5891, unsupervised posterior = -1.49463, MMD = 20.184\n",
      "Epoch 5: batch loss = 26.1406 and global loss = 33.7444\n",
      "KL_z1 = 2.17268, KL_z2 = 0.360496, RL = -17.3112, unsupervised posterior = -15.8222, MMD = 11.1899\n",
      "The CV global loss and reconstruction loss = 27.6287 and -18.2112\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = -4.26042 and global loss = -4.17417\n",
      "KL_z1 = 0.776859, KL_z2 = 0.375039, RL = -16.4206, unsupervised posterior = -22.7726, MMD = 0.0992997\n",
      "Epoch 5: batch loss = -6.92023 and global loss = -6.8405\n",
      "KL_z1 = 0.124588, KL_z2 = 0.00993532, RL = -15.7821, unsupervised posterior = -22.7726, MMD = 0.00387571\n",
      "The CV global loss and reconstruction loss = -7.16216 and -15.5052\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = -5.23876 and global loss = -4.70929\n",
      "KL_z1 = 1.94005, KL_z2 = 0.22594, RL = -19.6267, unsupervised posterior = -22.8647, MMD = 0.0685467\n",
      "Epoch 5: batch loss = -7.20543 and global loss = -6.84749\n",
      "KL_z1 = 0.0770212, KL_z2 = 0.0315563, RL = -15.3639, unsupervised posterior = -22.8647, MMD = 0.00809745\n",
      "The CV global loss and reconstruction loss = -7.33926 and -15.4783\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = -4.93982 and global loss = -3.94937\n",
      "KL_z1 = 0.31467, KL_z2 = 0.400327, RL = -16.5652, unsupervised posterior = -22.9798, MMD = 0.101933\n",
      "Epoch 5: batch loss = -6.87573 and global loss = -7.18949\n",
      "KL_z1 = 1.19898, KL_z2 = 0.0128903, RL = -15.4771, unsupervised posterior = -22.9798, MMD = 0.0055838\n",
      "The CV global loss and reconstruction loss = -7.35497 and -15.4638\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = -2.23813 and global loss = -4.36128\n",
      "KL_z1 = 0.530862, KL_z2 = 0.536082, RL = -17.1562, unsupervised posterior = -22.9338, MMD = 0.765928\n",
      "Epoch 5: batch loss = -6.87627 and global loss = -6.74551\n",
      "KL_z1 = 0.192081, KL_z2 = 0.0110498, RL = -16.014, unsupervised posterior = -22.9338, MMD = 0.0150536\n",
      "The CV global loss and reconstruction loss = -7.27578 and -15.5037\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = -4.30339 and global loss = -3.05419\n",
      "KL_z1 = 1.28388, KL_z2 = 0.459694, RL = -16.9581, unsupervised posterior = -22.7726, MMD = 0.599586\n",
      "Epoch 5: batch loss = -6.93134 and global loss = -6.87212\n",
      "KL_z1 = 1.09208, KL_z2 = 0.0492621, RL = -15.7497, unsupervised posterior = -22.7726, MMD = 0.0734376\n",
      "The CV global loss and reconstruction loss = -7.24628 and -15.4506\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = -2.90044 and global loss = -4.26837\n",
      "KL_z1 = 0.733967, KL_z2 = 0.255534, RL = -16.5528, unsupervised posterior = -22.8854, MMD = 0.866756\n",
      "Epoch 5: batch loss = -6.65112 and global loss = -7.00472\n",
      "KL_z1 = 0.381067, KL_z2 = 0.0223009, RL = -15.9679, unsupervised posterior = -22.8877, MMD = 0.0703233\n",
      "The CV global loss and reconstruction loss = -7.22359 and -15.4478\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 0.604703 and global loss = -0.548398\n",
      "KL_z1 = 2.42688, KL_z2 = 0.222553, RL = -16.7445, unsupervised posterior = -22.9338, MMD = 0.491857\n",
      "Epoch 5: batch loss = -7.0988 and global loss = -5.57298\n",
      "KL_z1 = 0.727917, KL_z2 = 0.0360972, RL = -15.4065, unsupervised posterior = -22.9333, MMD = 0.621088\n",
      "The CV global loss and reconstruction loss = -6.85382 and -15.4804\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 0.838685 and global loss = 1.25446\n",
      "KL_z1 = 3.59075, KL_z2 = 0.364152, RL = -18.5597, unsupervised posterior = -22.8877, MMD = 1.76553\n",
      "Epoch 5: batch loss = -6.99756 and global loss = -6.99938\n",
      "KL_z1 = 0.247671, KL_z2 = 0.0694801, RL = -15.4253, unsupervised posterior = -22.8877, MMD = 0.340386\n",
      "The CV global loss and reconstruction loss = -6.75329 and -15.6531\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 1000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = -0.85985 and global loss = 2.52443\n",
      "KL_z1 = 2.24949, KL_z2 = 0.203143, RL = -16.225, unsupervised posterior = -22.9337, MMD = 1.80988\n",
      "Epoch 5: batch loss = -6.49755 and global loss = -7.09765\n",
      "KL_z1 = 0.349685, KL_z2 = 0.0557323, RL = -15.8669, unsupervised posterior = -22.9337, MMD = 0.456747\n",
      "The CV global loss and reconstruction loss = -7.10861 and -15.4529\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = 48.7766 and global loss = 45.9527\n",
      "KL_z1 = 3.3303, KL_z2 = 0.838456, RL = -44.8227, unsupervised posterior = -2.58212, MMD = 0.238105\n",
      "Epoch 5: batch loss = 0.521877 and global loss = -3.9989\n",
      "KL_z1 = 1.24396, KL_z2 = 0.425877, RL = -18.2164, unsupervised posterior = -22.6535, MMD = 0.0874332\n",
      "The CV global loss and reconstruction loss = -2.78778 and -18.0147\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = 47.2698 and global loss = 46.9555\n",
      "KL_z1 = 1.43278, KL_z2 = 1.51821, RL = -51.2272, unsupervised posterior = -2.05141, MMD = 0.201758\n",
      "Epoch 5: batch loss = 0.626839 and global loss = -3.11858\n",
      "KL_z1 = 1.67588, KL_z2 = 0.785318, RL = -19.2832, unsupervised posterior = -22.8766, MMD = 0.156483\n",
      "The CV global loss and reconstruction loss = -4.47557 and -17.4142\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 47.4908 and global loss = 47.5818\n",
      "KL_z1 = 1.0107, KL_z2 = 0.375556, RL = -44.7384, unsupervised posterior = -0.839502, MMD = 0.178077\n",
      "Epoch 5: batch loss = -2.90671 and global loss = -2.11097\n",
      "KL_z1 = 1.84468, KL_z2 = 1.86782, RL = -17.0547, unsupervised posterior = -22.8881, MMD = 0.191447\n",
      "The CV global loss and reconstruction loss = -2.9076 and -16.5316\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = 53.7369 and global loss = 50.8009\n",
      "KL_z1 = 0.752341, KL_z2 = 0.682461, RL = -48.1188, unsupervised posterior = -0.90067, MMD = 1.60315\n",
      "Epoch 5: batch loss = -2.39724 and global loss = -2.2778\n",
      "KL_z1 = 2.87314, KL_z2 = 1.1945, RL = -17.0063, unsupervised posterior = -22.8927, MMD = 1.06387\n",
      "The CV global loss and reconstruction loss = -3.93111 and -17.3429\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = 49.9431 and global loss = 51.5478\n",
      "KL_z1 = 2.83979, KL_z2 = 1.99275, RL = -46.7365, unsupervised posterior = -1.5842, MMD = 1.56119\n",
      "Epoch 5: batch loss = -1.65363 and global loss = -0.148365\n",
      "KL_z1 = 1.33986, KL_z2 = 0.457903, RL = -16.5796, unsupervised posterior = -22.8674, MMD = 1.26185\n",
      "The CV global loss and reconstruction loss = -3.04925 and -18.0278\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = 49.8631 and global loss = 49.0915\n",
      "KL_z1 = 1.17952, KL_z2 = 0.953187, RL = -47.8212, unsupervised posterior = -0.803488, MMD = 1.84711\n",
      "Epoch 5: batch loss = -2.77364 and global loss = -2.08986\n",
      "KL_z1 = 2.26836, KL_z2 = 1.10866, RL = -17.9109, unsupervised posterior = -22.8584, MMD = 1.08883\n",
      "The CV global loss and reconstruction loss = -4.34705 and -16.8029\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 10.0, D = 50:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: batch loss = 65.577 and global loss = 68.0063\n",
      "KL_z1 = 2.76635, KL_z2 = 1.63442, RL = -45.15, unsupervised posterior = -0.492853, MMD = 15.1935\n",
      "Epoch 5: batch loss = 2.1984 and global loss = 4.6588\n",
      "KL_z1 = 4.08633, KL_z2 = 0.395608, RL = -17.2317, unsupervised posterior = -22.3897, MMD = 3.29228\n",
      "The CV global loss and reconstruction loss = -1.08559 and -16.6318\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 66.9316 and global loss = 70.0623\n",
      "KL_z1 = 3.21174, KL_z2 = 1.30264, RL = -49.3665, unsupervised posterior = -0.646721, MMD = 21.2156\n",
      "Epoch 5: batch loss = 11.3352 and global loss = 10.1904\n",
      "KL_z1 = 3.10421, KL_z2 = 1.06255, RL = -17.5737, unsupervised posterior = -22.8958, MMD = 10.8953\n",
      "The CV global loss and reconstruction loss = 1.97933 and -16.4106\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 1.0, batch size = 10000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 61.5134 and global loss = 66.9902\n",
      "KL_z1 = 1.63851, KL_z2 = 2.95262, RL = -51.0811, unsupervised posterior = -1.57008, MMD = 18.4095\n",
      "Epoch 5: batch loss = 7.30565 and global loss = 4.79988\n",
      "KL_z1 = 2.52586, KL_z2 = 0.300139, RL = -16.536, unsupervised posterior = -22.8895, MMD = 7.21189\n",
      "The CV global loss and reconstruction loss = -0.994073 and -16.2848\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = -208.634 and global loss = -204.14\n",
      "KL_z1 = 8.97946, KL_z2 = 0.914885, RL = -18.0338, unsupervised posterior = -22.8647, MMD = 0.106266\n",
      "Epoch 5: batch loss = -212.639 and global loss = -212.886\n",
      "KL_z1 = 0.0869098, KL_z2 = 0.0393145, RL = -15.785, unsupervised posterior = -22.8647, MMD = 0.0158339\n",
      "The CV global loss and reconstruction loss = -211.136 and -15.9766\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = -205.207 and global loss = -208.56\n",
      "KL_z1 = 1.97312, KL_z2 = 1.88761, RL = -16.4196, unsupervised posterior = -22.8877, MMD = 0.115556\n",
      "Epoch 5: batch loss = -213.065 and global loss = -212.867\n",
      "KL_z1 = 0.157174, KL_z2 = 0.0602366, RL = -15.654, unsupervised posterior = -22.8877, MMD = 0.0490301\n",
      "The CV global loss and reconstruction loss = -213.001 and -15.4461\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = -210.342 and global loss = -208.525\n",
      "KL_z1 = 3.46936, KL_z2 = 0.975854, RL = -16.7286, unsupervised posterior = -22.9795, MMD = 0.126629\n",
      "Epoch 5: batch loss = -214.069 and global loss = -212.87\n",
      "KL_z1 = 0.159811, KL_z2 = 0.0254421, RL = -15.5229, unsupervised posterior = -22.9798, MMD = 0.00925314\n",
      "The CV global loss and reconstruction loss = -213.242 and -15.4423\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = -206.173 and global loss = -206.715\n",
      "KL_z1 = 1.90248, KL_z2 = 0.855464, RL = -16.4466, unsupervised posterior = -22.8417, MMD = 1.14916\n",
      "Epoch 5: batch loss = -212.606 and global loss = -212.439\n",
      "KL_z1 = 0.203964, KL_z2 = 0.0780835, RL = -15.4695, unsupervised posterior = -22.8417, MMD = 0.105551\n",
      "The CV global loss and reconstruction loss = -213.181 and -15.4552\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = -205.312 and global loss = -209.473\n",
      "KL_z1 = 3.23777, KL_z2 = 1.05129, RL = -16.8711, unsupervised posterior = -22.8186, MMD = 0.730146\n",
      "Epoch 5: batch loss = -211.867 and global loss = -212.287\n",
      "KL_z1 = 1.31897, KL_z2 = 0.0372841, RL = -16.4513, unsupervised posterior = -22.8186, MMD = 0.366748\n",
      "The CV global loss and reconstruction loss = -213.105 and -15.4887\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = -210.406 and global loss = -206.661\n",
      "KL_z1 = 1.298, KL_z2 = 1.63982, RL = -17.3072, unsupervised posterior = -22.9568, MMD = 1.17384\n",
      "Epoch 5: batch loss = -212.359 and global loss = -212.527\n",
      "KL_z1 = 0.17416, KL_z2 = 0.0160141, RL = -15.2451, unsupervised posterior = -22.9568, MMD = 0.0349982\n",
      "The CV global loss and reconstruction loss = -213.161 and -15.4472\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = -204.574 and global loss = -204.936\n",
      "KL_z1 = 5.74614, KL_z2 = 0.358508, RL = -17.3288, unsupervised posterior = -22.9337, MMD = 4.26067\n",
      "Epoch 5: batch loss = -213.337 and global loss = -211.491\n",
      "KL_z1 = 0.420687, KL_z2 = 0.0558809, RL = -15.3878, unsupervised posterior = -22.9338, MMD = 0.644089\n",
      "The CV global loss and reconstruction loss = -212.665 and -15.4631\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = -200.527 and global loss = -200.294\n",
      "KL_z1 = 4.05808, KL_z2 = 1.71627, RL = -16.4766, unsupervised posterior = -22.9338, MMD = 7.19684\n",
      "Epoch 5: batch loss = -213.281 and global loss = -211.977\n",
      "KL_z1 = 2.11977, KL_z2 = 0.0811594, RL = -16.2739, unsupervised posterior = -22.9338, MMD = 1.68679\n",
      "The CV global loss and reconstruction loss = -212.032 and -15.5905\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 1000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = -203.827 and global loss = -203.327\n",
      "KL_z1 = 2.63945, KL_z2 = 0.780832, RL = -16.7273, unsupervised posterior = -22.9107, MMD = 5.12992\n",
      "Epoch 5: batch loss = -212.07 and global loss = -212.343\n",
      "KL_z1 = 0.493704, KL_z2 = 0.151384, RL = -15.4425, unsupervised posterior = -22.9107, MMD = 0.547708\n",
      "The CV global loss and reconstruction loss = -213.073 and -15.4569\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = 48.4927 and global loss = 43.2145\n",
      "KL_z1 = 2.44226, KL_z2 = 2.01196, RL = -46.2229, unsupervised posterior = -0.505362, MMD = 0.215638\n",
      "Epoch 5: batch loss = -202.38 and global loss = -200.663\n",
      "KL_z1 = 7.8115, KL_z2 = 2.5719, RL = -20.1526, unsupervised posterior = -22.8738, MMD = 0.183657\n",
      "The CV global loss and reconstruction loss = -202.45 and -18.3005\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = 46.7084 and global loss = 47.7456\n",
      "KL_z1 = 0.955541, KL_z2 = 0.968334, RL = -49.6322, unsupervised posterior = -0.656275, MMD = 0.191645\n",
      "Epoch 5: batch loss = -200.234 and global loss = -200.241\n",
      "KL_z1 = 6.68352, KL_z2 = 1.44722, RL = -18.4348, unsupervised posterior = -22.896, MMD = 0.127897\n",
      "The CV global loss and reconstruction loss = -201.819 and -19.5859\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 34.0357 and global loss = 25.4523\n",
      "KL_z1 = 0.917613, KL_z2 = 1.67752, RL = -47.577, unsupervised posterior = -1.73082, MMD = 0.216418\n",
      "Epoch 5: batch loss = -205.963 and global loss = -199.055\n",
      "KL_z1 = 2.71311, KL_z2 = 1.28491, RL = -17.8504, unsupervised posterior = -22.5831, MMD = 0.136411\n",
      "The CV global loss and reconstruction loss = -209.221 and -17.1006\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = 37.6349 and global loss = 28.8913\n",
      "KL_z1 = 2.3541, KL_z2 = 0.747921, RL = -51.5646, unsupervised posterior = -2.12402, MMD = 1.70724\n",
      "Epoch 5: batch loss = -201.168 and global loss = -202.614\n",
      "KL_z1 = 2.42288, KL_z2 = 2.9063, RL = -17.1901, unsupervised posterior = -22.862, MMD = 1.3756\n",
      "The CV global loss and reconstruction loss = -205.861 and -16.8338\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = 21.9928 and global loss = 21.5856\n",
      "KL_z1 = 2.94042, KL_z2 = 1.28956, RL = -45.896, unsupervised posterior = -3.82465, MMD = 1.9232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: batch loss = -198.242 and global loss = -198.881\n",
      "KL_z1 = 9.08508, KL_z2 = 3.02341, RL = -17.184, unsupervised posterior = -22.8605, MMD = 1.00402\n",
      "The CV global loss and reconstruction loss = -208.098 and -16.61\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = 34.8989 and global loss = 43.2238\n",
      "KL_z1 = 1.24602, KL_z2 = 1.75024, RL = -49.1119, unsupervised posterior = -1.82681, MMD = 1.90934\n",
      "Epoch 5: batch loss = -202.471 and global loss = -200.011\n",
      "KL_z1 = 4.07635, KL_z2 = 1.80675, RL = -16.4683, unsupervised posterior = -22.8944, MMD = 1.4164\n",
      "The CV global loss and reconstruction loss = -206.959 and -17.1436\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 62.2761 and global loss = 58.96\n",
      "KL_z1 = 2.13182, KL_z2 = 1.43506, RL = -46.3055, unsupervised posterior = -1.68963, MMD = 16.6837\n",
      "Epoch 5: batch loss = -189.827 and global loss = -202.197\n",
      "KL_z1 = 3.85779, KL_z2 = 1.26185, RL = -16.5348, unsupervised posterior = -22.8803, MMD = 7.76276\n",
      "The CV global loss and reconstruction loss = -204.12 and -17.0794\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 54.0265 and global loss = 47.9503\n",
      "KL_z1 = 2.08004, KL_z2 = 1.19138, RL = -46.9158, unsupervised posterior = -1.51382, MMD = 19.5509\n",
      "Epoch 5: batch loss = -203.457 and global loss = -193.979\n",
      "KL_z1 = 5.93691, KL_z2 = 1.24345, RL = -16.8768, unsupervised posterior = -22.8473, MMD = 9.62288\n",
      "The CV global loss and reconstruction loss = -204.873 and -16.5731\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 10.0, batch size = 10000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 55.3524 and global loss = 62.8554\n",
      "KL_z1 = 2.42228, KL_z2 = 1.89216, RL = -47.1837, unsupervised posterior = -0.951888, MMD = 18.753\n",
      "Epoch 5: batch loss = -198.731 and global loss = -194.779\n",
      "KL_z1 = 9.78067, KL_z2 = 1.16209, RL = -19.5134, unsupervised posterior = -22.8349, MMD = 8.49722\n",
      "The CV global loss and reconstruction loss = -202.722 and -16.3929\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = 13.3754 and global loss = 13.5502\n",
      "KL_z1 = 0.169996, KL_z2 = 0.0796729, RL = -15.5672, unsupervised posterior = -23.0028, MMD = 0.0228459\n",
      "Epoch 5: batch loss = 13.1308 and global loss = 13.1528\n",
      "KL_z1 = 0.000713726, KL_z2 = 1.57356e-05, RL = -15.5115, unsupervised posterior = -23.0028, MMD = 0.0020849\n",
      "The CV global loss and reconstruction loss = 13.1578 and -15.4443\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = 13.654 and global loss = 13.5044\n",
      "KL_z1 = 0.0867923, KL_z2 = 0.00852214, RL = -15.4994, unsupervised posterior = -22.9568, MMD = 0.0120744\n",
      "Epoch 5: batch loss = 13.107 and global loss = 13.1669\n",
      "KL_z1 = 0.00602057, KL_z2 = 1.43051e-06, RL = -15.3956, unsupervised posterior = -22.9568, MMD = 0.000409136\n",
      "The CV global loss and reconstruction loss = 13.1675 and -15.4537\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 13.5087 and global loss = 13.3101\n",
      "KL_z1 = 0.0924091, KL_z2 = 0.0146357, RL = -16.0802, unsupervised posterior = -22.7956, MMD = 0.0412783\n",
      "Epoch 5: batch loss = 13.2907 and global loss = 13.1751\n",
      "KL_z1 = 0.00601928, KL_z2 = 8.82149e-06, RL = -15.4757, unsupervised posterior = -22.7956, MMD = 0.00376013\n",
      "The CV global loss and reconstruction loss = 13.1698 and -15.4527\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = 13.5497 and global loss = 13.6656\n",
      "KL_z1 = 0.223421, KL_z2 = 0.0118292, RL = -15.385, unsupervised posterior = -22.8877, MMD = 0.105517\n",
      "Epoch 5: batch loss = 13.1959 and global loss = 13.2249\n",
      "KL_z1 = 0.0270155, KL_z2 = 3.66569e-05, RL = -15.3478, unsupervised posterior = -22.8877, MMD = 0.101908\n",
      "The CV global loss and reconstruction loss = 13.1708 and -15.4392\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = 13.6664 and global loss = 14.9765\n",
      "KL_z1 = 0.114575, KL_z2 = 0.029082, RL = -15.4472, unsupervised posterior = -23.0258, MMD = 0.437109\n",
      "Epoch 5: batch loss = 13.1822 and global loss = 13.2186\n",
      "KL_z1 = 0.0114635, KL_z2 = 4.74453e-05, RL = -15.416, unsupervised posterior = -23.0259, MMD = 0.0010765\n",
      "The CV global loss and reconstruction loss = 13.181 and -15.4659\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = 13.7663 and global loss = 13.4096\n",
      "KL_z1 = 0.0886232, KL_z2 = 0.0281197, RL = -15.6991, unsupervised posterior = -22.7956, MMD = 0.139945\n",
      "Epoch 5: batch loss = 13.292 and global loss = 13.1547\n",
      "KL_z1 = 0.00119022, KL_z2 = 3.09944e-06, RL = -15.7284, unsupervised posterior = -22.7956, MMD = 0.00305829\n",
      "The CV global loss and reconstruction loss = 13.1631 and -15.4442\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 13.5608 and global loss = 14.425\n",
      "KL_z1 = 0.832561, KL_z2 = 0.028032, RL = -15.5437, unsupervised posterior = -22.9568, MMD = 0.346728\n",
      "Epoch 5: batch loss = 13.3271 and global loss = 13.2058\n",
      "KL_z1 = 0.029526, KL_z2 = 2.74479e-05, RL = -15.5397, unsupervised posterior = -22.9568, MMD = 0.0203059\n",
      "The CV global loss and reconstruction loss = 13.266 and -15.5464\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 14.8382 and global loss = 14.5328\n",
      "KL_z1 = 0.869168, KL_z2 = 0.0626399, RL = -15.6724, unsupervised posterior = -23.0028, MMD = 1.50865\n",
      "Epoch 5: batch loss = 13.164 and global loss = 13.2014\n",
      "KL_z1 = 0.0413179, KL_z2 = 0.00014279, RL = -15.438, unsupervised posterior = -23.0028, MMD = 0.022655\n",
      "The CV global loss and reconstruction loss = 13.2401 and -15.5132\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 1000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 15.4153 and global loss = 15.434\n",
      "KL_z1 = 0.586564, KL_z2 = 0.0389662, RL = -15.5147, unsupervised posterior = -22.7726, MMD = 0.377281\n",
      "Epoch 5: batch loss = 13.1016 and global loss = 13.1496\n",
      "KL_z1 = 0.00392277, KL_z2 = 4.29451e-05, RL = -15.3734, unsupervised posterior = -22.7726, MMD = 0.0084001\n",
      "The CV global loss and reconstruction loss = 13.2637 and -15.499\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = 16.6131 and global loss = 19.209\n",
      "KL_z1 = 3.49251, KL_z2 = 1.00917, RL = -18.5367, unsupervised posterior = -12.5251, MMD = 0.198157\n",
      "Epoch 5: batch loss = 13.7825 and global loss = 15.5261\n",
      "KL_z1 = 0.148506, KL_z2 = 0.124846, RL = -15.6553, unsupervised posterior = -22.8997, MMD = 0.0281335\n",
      "The CV global loss and reconstruction loss = 13.3787 and -15.4868\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = 16.87 and global loss = 18.6455\n",
      "KL_z1 = 1.22156, KL_z2 = 0.995306, RL = -17.3698, unsupervised posterior = -13.534, MMD = 0.119667\n",
      "Epoch 5: batch loss = 13.9465 and global loss = 13.5439\n",
      "KL_z1 = 0.610388, KL_z2 = 0.0763639, RL = -15.6889, unsupervised posterior = -22.8697, MMD = 0.0585101\n",
      "The CV global loss and reconstruction loss = 13.4894 and -15.5787\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 21.9111 and global loss = 17.9447\n",
      "KL_z1 = 1.67123, KL_z2 = 1.39468, RL = -17.2984, unsupervised posterior = -13.4082, MMD = 0.171665\n",
      "Epoch 5: batch loss = 14.3373 and global loss = 13.7258\n",
      "KL_z1 = 0.158844, KL_z2 = 0.0727154, RL = -15.66, unsupervised posterior = -21.8042, MMD = 0.113344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV global loss and reconstruction loss = 13.609 and -15.6428\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = 20.6911 and global loss = 18.806\n",
      "KL_z1 = 2.16682, KL_z2 = 1.25469, RL = -17.6142, unsupervised posterior = -22.9041, MMD = 2.03684\n",
      "Epoch 5: batch loss = 13.9331 and global loss = 15.1147\n",
      "KL_z1 = 0.279875, KL_z2 = 0.0219235, RL = -15.556, unsupervised posterior = -22.9066, MMD = 0.233791\n",
      "The CV global loss and reconstruction loss = 13.6264 and -15.5034\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = 20.3178 and global loss = 20.1638\n",
      "KL_z1 = 1.01897, KL_z2 = 2.83125, RL = -17.0913, unsupervised posterior = -22.8671, MMD = 1.70481\n",
      "Epoch 5: batch loss = 16.1237 and global loss = 14.6686\n",
      "KL_z1 = 0.227216, KL_z2 = 0.0508351, RL = -15.9878, unsupervised posterior = -22.8674, MMD = 0.0739456\n",
      "The CV global loss and reconstruction loss = 16.0495 and -17.9466\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = 21.5872 and global loss = 20.3294\n",
      "KL_z1 = 0.611577, KL_z2 = 0.893509, RL = -27.6936, unsupervised posterior = -22.8563, MMD = 1.49093\n",
      "Epoch 5: batch loss = 13.7444 and global loss = 14.0221\n",
      "KL_z1 = 0.342622, KL_z2 = 0.0296419, RL = -15.5611, unsupervised posterior = -22.8651, MMD = 0.223896\n",
      "The CV global loss and reconstruction loss = 13.4275 and -15.534\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 25.8402 and global loss = 37.9898\n",
      "KL_z1 = 2.36825, KL_z2 = 0.93742, RL = -18.0343, unsupervised posterior = -10.747, MMD = 15.3877\n",
      "Epoch 5: batch loss = 16.3718 and global loss = 16.123\n",
      "KL_z1 = 0.849696, KL_z2 = 0.0668527, RL = -15.4651, unsupervised posterior = -22.9066, MMD = 0.884487\n",
      "The CV global loss and reconstruction loss = 14.8929 and -15.6112\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 26.0442 and global loss = 37.4614\n",
      "KL_z1 = 1.85044, KL_z2 = 0.335182, RL = -17.9837, unsupervised posterior = -20.7795, MMD = 6.96215\n",
      "Epoch 5: batch loss = 16.5812 and global loss = 14.8939\n",
      "KL_z1 = 0.969126, KL_z2 = 0.0799941, RL = -16.3361, unsupervised posterior = -22.8927, MMD = 0.410648\n",
      "The CV global loss and reconstruction loss = 14.6201 and -15.7642\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 0.1, batch size = 10000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 38.3253 and global loss = 36.7175\n",
      "KL_z1 = 2.45239, KL_z2 = 0.563522, RL = -16.7024, unsupervised posterior = -22.8445, MMD = 4.69185\n",
      "Epoch 5: batch loss = 16.2903 and global loss = 18.3047\n",
      "KL_z1 = 0.478088, KL_z2 = 0.10296, RL = -17.0893, unsupervised posterior = -22.8835, MMD = 0.916091\n",
      "The CV global loss and reconstruction loss = 15.4299 and -15.5324\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = -6.74664 and global loss = -6.92661\n",
      "KL_z1 = 0.351128, KL_z2 = 0.00678481, RL = -15.9955, unsupervised posterior = -22.8417, MMD = 0.0110835\n",
      "Epoch 5: batch loss = -7.41169 and global loss = -7.4262\n",
      "KL_z1 = 0.00414475, KL_z2 = 0.00270872, RL = -15.5076, unsupervised posterior = -22.8417, MMD = 0.0121977\n",
      "The CV global loss and reconstruction loss = -7.38695 and -15.4817\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = -7.14074 and global loss = -6.51263\n",
      "KL_z1 = 0.400702, KL_z2 = 0.00816404, RL = -15.8014, unsupervised posterior = -22.7956, MMD = 0.0178333\n",
      "Epoch 5: batch loss = -7.38708 and global loss = -7.30737\n",
      "KL_z1 = 0.00137531, KL_z2 = 1.44243e-05, RL = -15.4837, unsupervised posterior = -22.7956, MMD = 0.00355263\n",
      "The CV global loss and reconstruction loss = -7.4166 and -15.4552\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = -5.9947 and global loss = -7.08036\n",
      "KL_z1 = 0.404484, KL_z2 = 0.00807257, RL = -16.1305, unsupervised posterior = -22.8417, MMD = 0.00947635\n",
      "Epoch 5: batch loss = -7.24813 and global loss = -7.38021\n",
      "KL_z1 = 0.000897328, KL_z2 = 1.07288e-06, RL = -15.6343, unsupervised posterior = -22.8417, MMD = 0.00559007\n",
      "The CV global loss and reconstruction loss = -7.43205 and -15.4402\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = -6.95296 and global loss = -6.9132\n",
      "KL_z1 = 0.627938, KL_z2 = 0.0106568, RL = -15.5845, unsupervised posterior = -22.8417, MMD = 0.325424\n",
      "Epoch 5: batch loss = -7.2861 and global loss = -7.35269\n",
      "KL_z1 = 0.00362866, KL_z2 = 5.39422e-06, RL = -15.5317, unsupervised posterior = -22.8417, MMD = 0.00546871\n",
      "The CV global loss and reconstruction loss = -7.42942 and -15.4425\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = -5.91696 and global loss = -6.95704\n",
      "KL_z1 = 0.28289, KL_z2 = 0.0607964, RL = -16.3162, unsupervised posterior = -22.8417, MMD = 0.140543\n",
      "Epoch 5: batch loss = -7.30521 and global loss = -7.42505\n",
      "KL_z1 = 0.00138327, KL_z2 = 3.72529e-06, RL = -15.5048, unsupervised posterior = -22.8417, MMD = 0.0869258\n",
      "The CV global loss and reconstruction loss = -7.40014 and -15.4632\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = -6.88292 and global loss = -7.0271\n",
      "KL_z1 = 0.513285, KL_z2 = 0.00619532, RL = -15.7166, unsupervised posterior = -22.8417, MMD = 0.365227\n",
      "Epoch 5: batch loss = -7.25491 and global loss = -7.40862\n",
      "KL_z1 = 0.00706335, KL_z2 = 8.43406e-06, RL = -15.5553, unsupervised posterior = -22.8417, MMD = 0.00314467\n",
      "The CV global loss and reconstruction loss = -7.41033 and -15.4393\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = -5.99572 and global loss = -6.55192\n",
      "KL_z1 = 0.611602, KL_z2 = 0.035667, RL = -15.521, unsupervised posterior = -22.9107, MMD = 0.305306\n",
      "Epoch 5: batch loss = -7.43019 and global loss = -7.37937\n",
      "KL_z1 = 0.0242542, KL_z2 = 7.18832e-05, RL = -15.451, unsupervised posterior = -22.9107, MMD = 0.007563\n",
      "The CV global loss and reconstruction loss = -7.39651 and -15.4567\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = -6.75789 and global loss = -5.79959\n",
      "KL_z1 = 0.479201, KL_z2 = 0.048258, RL = -15.6292, unsupervised posterior = -22.8647, MMD = 0.533571\n",
      "Epoch 5: batch loss = -7.54359 and global loss = -7.40961\n",
      "KL_z1 = 0.00208048, KL_z2 = 2.43485e-05, RL = -15.2896, unsupervised posterior = -22.8647, MMD = 0.0116676\n",
      "The CV global loss and reconstruction loss = -7.4225 and -15.4428\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 1000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = -3.66148 and global loss = -6.70166\n",
      "KL_z1 = 0.459436, KL_z2 = 0.0675744, RL = -15.5204, unsupervised posterior = -22.8417, MMD = 0.559158\n",
      "Epoch 5: batch loss = -7.20335 and global loss = -7.39237\n",
      "KL_z1 = 0.0747657, KL_z2 = 4.72665e-05, RL = -15.5035, unsupervised posterior = -22.8417, MMD = 0.058159\n",
      "The CV global loss and reconstruction loss = -7.40752 and -15.4566\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = 1.97225 and global loss = 8.49674\n",
      "KL_z1 = 4.97057, KL_z2 = 0.54206, RL = -23.2026, unsupervised posterior = -22.8881, MMD = 0.183999\n",
      "Epoch 5: batch loss = -6.97244 and global loss = -6.97323\n",
      "KL_z1 = 0.268544, KL_z2 = 0.603231, RL = -15.5872, unsupervised posterior = -22.8881, MMD = 0.11061\n",
      "The CV global loss and reconstruction loss = -6.37667 and -15.6081\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 0.1, D = 100:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: batch loss = -0.0880454 and global loss = -0.342481\n",
      "KL_z1 = 3.78462, KL_z2 = 1.9759, RL = -18.7131, unsupervised posterior = -22.8839, MMD = 0.10516\n",
      "Epoch 5: batch loss = -5.25044 and global loss = -6.1226\n",
      "KL_z1 = 0.326463, KL_z2 = 0.0468429, RL = -15.501, unsupervised posterior = -22.8997, MMD = 0.0526171\n",
      "The CV global loss and reconstruction loss = -7.15823 and -15.5654\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = 3.54277 and global loss = -0.0917754\n",
      "KL_z1 = 1.89419, KL_z2 = 0.345415, RL = -16.8521, unsupervised posterior = -22.8927, MMD = 0.125802\n",
      "Epoch 5: batch loss = -6.68856 and global loss = -6.41175\n",
      "KL_z1 = 0.656821, KL_z2 = 0.0241779, RL = -15.637, unsupervised posterior = -22.8927, MMD = 0.0438254\n",
      "The CV global loss and reconstruction loss = -5.86459 and -16.3646\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = -1.77623 and global loss = -1.2008\n",
      "KL_z1 = 3.82816, KL_z2 = 1.06207, RL = -17.6571, unsupervised posterior = -20.6363, MMD = 1.31556\n",
      "Epoch 5: batch loss = -6.07463 and global loss = -4.22349\n",
      "KL_z1 = 1.88971, KL_z2 = 0.0165746, RL = -15.5967, unsupervised posterior = -22.8973, MMD = 0.227061\n",
      "The CV global loss and reconstruction loss = -6.92189 and -15.6727\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = -1.7485 and global loss = -3.20649\n",
      "KL_z1 = 1.84526, KL_z2 = 3.41269, RL = -21.6377, unsupervised posterior = -22.8628, MMD = 1.58967\n",
      "Epoch 5: batch loss = -6.00118 and global loss = -6.0688\n",
      "KL_z1 = 0.657928, KL_z2 = 0.0320246, RL = -15.528, unsupervised posterior = -22.8628, MMD = 0.426817\n",
      "The CV global loss and reconstruction loss = -6.47502 and -15.5973\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = -1.21473 and global loss = 0.451349\n",
      "KL_z1 = 2.50084, KL_z2 = 1.58368, RL = -16.7063, unsupervised posterior = -22.1954, MMD = 1.58805\n",
      "Epoch 5: batch loss = -5.82976 and global loss = -5.52603\n",
      "KL_z1 = 0.272493, KL_z2 = 0.0709911, RL = -15.5863, unsupervised posterior = -22.872, MMD = 0.242589\n",
      "The CV global loss and reconstruction loss = -6.30429 and -15.511\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = 6.84164 and global loss = 13.514\n",
      "KL_z1 = 3.533, KL_z2 = 0.451069, RL = -17.1179, unsupervised posterior = -22.8352, MMD = 11.3731\n",
      "Epoch 5: batch loss = -3.44708 and global loss = -2.64919\n",
      "KL_z1 = 1.36551, KL_z2 = 0.0635581, RL = -15.7347, unsupervised posterior = -22.8352, MMD = 1.15545\n",
      "The CV global loss and reconstruction loss = -4.04605 and -15.4657\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = 12.1218 and global loss = 10.9321\n",
      "KL_z1 = 3.18268, KL_z2 = 0.804122, RL = -17.8841, unsupervised posterior = -21.8225, MMD = 9.83874\n",
      "Epoch 5: batch loss = -2.35983 and global loss = -5.78387\n",
      "KL_z1 = 1.62016, KL_z2 = 0.11026, RL = -15.8789, unsupervised posterior = -22.8812, MMD = 1.478\n",
      "The CV global loss and reconstruction loss = -5.82476 and -15.4896\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 1.0, batch size = 10000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = 6.07482 and global loss = 7.3966\n",
      "KL_z1 = 4.20531, KL_z2 = 0.482938, RL = -18.8679, unsupervised posterior = -19.6125, MMD = 9.7021\n",
      "Epoch 5: batch loss = -3.23865 and global loss = -2.69414\n",
      "KL_z1 = 0.524552, KL_z2 = 0.144008, RL = -15.4908, unsupervised posterior = -22.8904, MMD = 2.23077\n",
      "The CV global loss and reconstruction loss = -6.48108 and -15.6189\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = -211.413 and global loss = -211.72\n",
      "KL_z1 = 0.692741, KL_z2 = 0.0375681, RL = -15.5758, unsupervised posterior = -22.8877, MMD = 0.95531\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = -212.18 and global loss = -211.236\n",
      "KL_z1 = 0.739156, KL_z2 = 0.0414134, RL = -15.8585, unsupervised posterior = -22.9798, MMD = 0.652497\n",
      "Epoch 5: batch loss = -214.161 and global loss = -212.987\n",
      "KL_z1 = 0.114286, KL_z2 = 8.69334e-05, RL = -15.3436, unsupervised posterior = -22.9798, MMD = 0.0958944\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = -211.547 and global loss = -210.909\n",
      "KL_z1 = 0.520607, KL_z2 = 0.0385792, RL = -15.868, unsupervised posterior = -22.8186, MMD = 1.17668\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 1000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 0.1, D = 50:\n",
      "Epoch 0: batch loss = -201.333 and global loss = -203.238\n",
      "KL_z1 = 4.18563, KL_z2 = 1.03833, RL = -16.8947, unsupervised posterior = -22.8789, MMD = 0.160381\n",
      "Epoch 5: batch loss = -188.713 and global loss = -201.772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL_z1 = 12.9626, KL_z2 = 0.230919, RL = -16.81, unsupervised posterior = -22.8789, MMD = 0.157585\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 0.1, D = 100:\n",
      "Epoch 0: batch loss = -201.656 and global loss = -198.146\n",
      "KL_z1 = 15.2347, KL_z2 = 3.95986, RL = -18.1107, unsupervised posterior = -22.8766, MMD = 0.191443\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 0.1, D = 200:\n",
      "Epoch 0: batch loss = -195.798 and global loss = -198.923\n",
      "KL_z1 = 6.7344, KL_z2 = 2.56773, RL = -20.5251, unsupervised posterior = -22.8651, MMD = 0.172831\n",
      "Epoch 5: batch loss = -210.394 and global loss = -210.726\n",
      "KL_z1 = 0.461506, KL_z2 = 0.0802013, RL = -15.6432, unsupervised posterior = -22.8651, MMD = 0.156306\n",
      "The CV global loss and reconstruction loss = -212.772 and -15.5892\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 1.0, D = 50:\n",
      "Epoch 0: batch loss = -174.011 and global loss = -200.04\n",
      "KL_z1 = 6.95067, KL_z2 = 3.84474, RL = -18.2679, unsupervised posterior = -22.8835, MMD = 1.92647\n",
      "Epoch 5: batch loss = -207.622 and global loss = -210.942\n",
      "KL_z1 = 3.89595, KL_z2 = 0.0747818, RL = -16.1504, unsupervised posterior = -22.8835, MMD = 1.34358\n",
      "The CV global loss and reconstruction loss = -208.553 and -16.3471\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 1.0, D = 100:\n",
      "Epoch 0: batch loss = -202.863 and global loss = -198.733\n",
      "KL_z1 = 10.9032, KL_z2 = 0.739294, RL = -20.8782, unsupervised posterior = -22.8628, MMD = 2.20115\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 1.0, D = 200:\n",
      "Epoch 0: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 10.0, D = 50:\n",
      "Epoch 0: batch loss = -171.136 and global loss = -172.196\n",
      "KL_z1 = 11.6, KL_z2 = 6.12661, RL = -18.1854, unsupervised posterior = -22.8513, MMD = 18.5616\n",
      "Epoch 5: batch loss = -208.268 and global loss = -209.691\n",
      "KL_z1 = 2.40447, KL_z2 = 0.065959, RL = -15.8362, unsupervised posterior = -22.8513, MMD = 0.259874\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 10.0, D = 100:\n",
      "Epoch 0: batch loss = -171.584 and global loss = -186.224\n",
      "KL_z1 = 11.3417, KL_z2 = 1.68391, RL = -23.7643, unsupervised posterior = -22.8766, MMD = 16.4387\n",
      "Epoch 5: batch loss = nan and global loss = nan\n",
      "KL_z1 = nan, KL_z2 = nan, RL = nan, unsupervised posterior = nan, MMD = nan\n",
      "The CV global loss and reconstruction loss = nan and nan\n",
      "\n",
      "\n",
      "Begin analysis for dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.01, alpha = 10.0, batch size = 10000, beta = 10.0, D = 200:\n",
      "Epoch 0: batch loss = -181.291 and global loss = -182.955\n",
      "KL_z1 = 4.75833, KL_z2 = 1.98377, RL = -17.5102, unsupervised posterior = -22.8674, MMD = 18.3206\n",
      "Epoch 5: batch loss = -206.007 and global loss = -200.859\n",
      "KL_z1 = 2.28222, KL_z2 = 0.126736, RL = -20.1975, unsupervised posterior = -22.8674, MMD = 0.586324\n",
      "The CV global loss and reconstruction loss = -212.388 and -15.4946\n",
      "\n",
      "\n",
      "The minimum loss of 15.437 was obtained using dims = [20, 20, 50, 50, 50, 50, 50], lr = 0.001, alpha = 0.1, bs = 1000, beta = 10.0, D = 50\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hidden units in the hidden layers\n",
    "# [z1, z2, encoding1, encoding2, decoding1, decoding2, unsupervised]\n",
    "hidden_dim_list = [\n",
    "    [20,20,50,50,50,50,50]\n",
    "]\n",
    "\n",
    "lr_list = [1e-3,1e-2]\n",
    "\n",
    "alpha_list = [1e-1,1e0,1e1]\n",
    "\n",
    "batch_size_list = [1000,10000]\n",
    "\n",
    "beta_list = [1e-1,1e0,1e1]\n",
    "\n",
    "D_list = [50,100,200]\n",
    "\n",
    "loss_min = 1e3\n",
    "\n",
    "for hd in hidden_dim_list:\n",
    "    \n",
    "    for lr in lr_list:\n",
    "        \n",
    "        for alpha in alpha_list:\n",
    "            \n",
    "            for bs in batch_size_list:\n",
    "                \n",
    "                for beta in beta_list:\n",
    "                    \n",
    "                    for D in D_list:\n",
    "        \n",
    "                        print(\"Begin analysis for dims = %s, lr = %s, alpha = %s, batch size = %s, beta = %s, D = %s:\"%(hd,lr,alpha,bs,beta,D))\n",
    "\n",
    "                        dims = {\n",
    "                            'x':VFAE_train_X.shape[1],\n",
    "                            'y_cat':len(np.unique(VFAE_train_Y)),\n",
    "                            's':VFAE_train_s.shape[1],\n",
    "                            'z1':hd[0],\n",
    "                            'z2':hd[1],\n",
    "                            'enc1_hid':hd[2],\n",
    "                            'enc2_hid':hd[3],\n",
    "                            'dec1_hid':hd[4],\n",
    "                            'dec2_hid':hd[5],\n",
    "                            'us_hid':hd[6]\n",
    "                        }\n",
    "\n",
    "                        params = initialize_params(dims,N_epochs=10,lr=lr,print_freq=5,batch_size=bs,alpha=alpha,beta=beta,D=D,gamma=1)\n",
    "                        weights, bias = initialize_weights_biases(params)\n",
    "                        enc_X, latent_X, loss, LH_x = train_VFAE(VFAE_train_X,VFAE_train_Y,VFAE_train_s,VFAE_CV_X,VFAE_CV_Y,VFAE_CV_s,weights,bias,params,dims)\n",
    "\n",
    "                        print(\"The CV global loss = %s and reconstruction loss and = %s\"%(loss,LH_x))\n",
    "                        print()\n",
    "                        print()\n",
    "\n",
    "                        if (np.abs(LH_x) < loss_min):\n",
    "                            loss_min = np.abs(LH_x)\n",
    "                            d_optimal = hd\n",
    "                            lr_optimal = lr\n",
    "                            alpha_optimal = alpha\n",
    "                            bs_optimal = bs\n",
    "                            beta_optimal = beta\n",
    "                            D_optimal = D\n",
    "                            latent_X_optimal = latent_X\n",
    "                    \n",
    "print(\"The minimum loss of %s was obtained using dims = %s, lr = %s, alpha = %s, bs = %s, beta = %s, D = %s\"%(loss_min,d_optimal,lr_optimal,alpha_optimal,bs_optimal,beta_optimal,D_optimal))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the VFAE with the test set using the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yevgenik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: batch loss = 15.5253 and global loss = 16.5807\n",
      "KL_z1 = 0.968424, KL_z2 = 0.384209, RL = -16.9511, unsupervised posterior = -22.8647, MMD = 1.01463\n",
      "Epoch 10: batch loss = 13.268 and global loss = 13.2127\n",
      "KL_z1 = 0.0505597, KL_z2 = 0.000966574, RL = -15.4028, unsupervised posterior = -22.8647, MMD = 0.00922411\n"
     ]
    }
   ],
   "source": [
    "# Run with the optimal params\n",
    "dims = {\n",
    "    'x':VFAE_train_X.shape[1],\n",
    "    'y_cat':len(np.unique(VFAE_train_Y)),\n",
    "    's':VFAE_train_s.shape[1],\n",
    "    'z1':d_optimal[0],\n",
    "    'z2':d_optimal[1],\n",
    "    'enc1_hid':d_optimal[2],\n",
    "    'enc2_hid':d_optimal[3],\n",
    "    'dec1_hid':d_optimal[4],\n",
    "    'dec2_hid':d_optimal[5],\n",
    "    'us_hid':d_optimal[6]\n",
    "}\n",
    "\n",
    "params = initialize_params(dims,N_epochs=20,lr=lr_optimal,print_freq=10,batch_size=bs_optimal,alpha=alpha_optimal,beta=beta_optimal,D=D_optimal,gamma=1.)\n",
    "weights, bias = initialize_weights_biases(params)\n",
    "enc_X, latent_X, loss, LH_x = train_VFAE(VFAE_train_X,VFAE_train_Y,VFAE_train_s,VFAE_test_X,VFAE_test_Y,VFAE_test_s,weights,bias,params,dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(np.arange(VFAE_test_X.shape[0]),test_size=0.3)\n",
    "clf_train_X, clf_test_X = VFAE_test_X[train_index], VFAE_test_X[test_index]\n",
    "clf_train_Y, clf_test_Y = VFAE_test_Y[train_index].ravel(), VFAE_test_Y[test_index].ravel()\n",
    "clf_train_s, clf_test_s = VFAE_test_s[train_index], VFAE_test_s[test_index]\n",
    "df_train, df_test = df_VFAE_test.loc[train_index], df_VFAE_test.loc[test_index]\n",
    "\n",
    "train_latent_X, test_latent_X = latent_X[train_index], latent_X[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.230942872026\n",
      "Precision score = 0.538461538462\n",
      "Recall score = 0.146993810787\n",
      "Accuracy score = 0.993615165603\n",
      "\n",
      "Discrimination_ratio = 1.8552219649734132\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa1305240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Full DF\n",
    "params = {'n_jobs': -1, 'n_estimators': 47, 'max_depth': 8, 'criterion': 'gini', 'class_weight': None}\n",
    "weights = np.ones(clf_train_Y.shape[0])\n",
    "weights[clf_train_Y==1] = 1.\n",
    "weights[clf_train_Y==0] = 0.5\n",
    "RFC = run_clf('RFC',train_latent_X,clf_train_Y,params,weights=weights)\n",
    "bias = check_error_and_discrimination(RFC,test_latent_X,clf_test_Y,df_test,sensitive_features)\n",
    "df_bias = pd.DataFrame(bias).transpose()\n",
    "df_bias.columns = [\"Group\",\"N_group/N_total\",\"N_group/N_total|(predicted arrest)\",\"N_group/N_total|(predicted not arrest)\"]\n",
    "plot_distribution(df_bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
