{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yevgenik/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, fbeta_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "f_half_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/OH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the columns that will be used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workable_cols = ['stop_date','stop_time','location_raw','driver_gender','driver_race','driver_race_raw','violation',\n",
    "                 'search_conducted','contraband_found','is_arrested','drugs_related_stop']\n",
    "\n",
    "df_reduced = df[workable_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert time into hours and date into month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced['stop_hour'] = df_reduced['stop_time'].apply(lambda x: x.split(':')[0])\n",
    "df_reduced['stop_month'] = df_reduced['stop_date'].apply(lambda x: x.split('-')[1])\n",
    "df_reduced.drop('stop_time',axis=1,inplace=True)\n",
    "df_reduced.drop('stop_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the 20 most frequent locations, which account for ~90% of all the locations. The remainder is set to null, to be imputed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_reduced['location_raw'].value_counts()[:20].sum()/df_reduced.shape[0])\n",
    "keep_locations = list(df_reduced['location_raw'].value_counts().keys()[:20])\n",
    "df_reduced['location_raw'] = df_reduced['location_raw'].apply(lambda x: int(x) if x in keep_locations else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two columns for driver race. Impute the 'other' values in one, with the values from the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced['driver_race'].loc[df_reduced['driver_race']=='Other'] = df_reduced['driver_race_raw'].loc[df_reduced['driver_race']=='Other']\n",
    "#df_reduced['driver_race'] = df_reduced.apply(lambda x: x['driver_race_raw'] if x['driver_race'] == 'Other' else x['driver_race'],axis=1)\n",
    "df_reduced.drop('driver_race_raw',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a realtively small number of unique violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "violations_unique = []\n",
    "for l in df_reduced['violation'].dropna().unique():\n",
    "    violations_unique = violations_unique + l.lower().split(',')\n",
    "violations_unique = set(violations_unique)\n",
    "print(violations_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping between violations to integer values, which are ranked from worst to least\n",
    "### The worst one will be selected in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def violations_denoter(x):\n",
    "    \n",
    "    violations_severity = {\n",
    "        'dui':0,\n",
    "        'speeding':1,\n",
    "        'stop sign/light':2,\n",
    "        'license':3,\n",
    "        'cell phone':4,\n",
    "        'paperwork':5,\n",
    "        'registration/plates':6,\n",
    "        'safe movement':7,\n",
    "        'seat belt':8,    \n",
    "        'equipment':9,\n",
    "        'lights':10,\n",
    "        'truck':11,\n",
    "        'other':12,\n",
    "        'other (non-mapped)':13\n",
    "    }\n",
    "    \n",
    "    violations = []\n",
    "    for k,v in violations_severity.items():\n",
    "        if (k in x.lower()):\n",
    "            violations.append(v)\n",
    "            \n",
    "    return min(violations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced['violations_numbered'] = df_reduced['violation'].fillna('other (non-mapped)').apply(violations_denoter)\n",
    "df_reduced.drop('violation',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = list(df_reduced.columns)\n",
    "categorical.remove('is_arrested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_na_categorical(df,column):\n",
    "\n",
    "    prob = dict(df[column].value_counts()/len(df))\n",
    "    \n",
    "    keys = list(prob.keys())\n",
    "    \n",
    "    sum_prob = sum(prob.values())\n",
    "\n",
    "    for k,v in prob.items():\n",
    "        prob[k] = prob[k]/sum_prob\n",
    "\n",
    "    prob_list = list(prob.values())\n",
    "\n",
    "    to_fillin = np.random.choice(keys, len(df[column].loc[df[column].isnull()]), p = prob_list)\n",
    "    \n",
    "    df[column].loc[df[column].isnull()] = to_fillin\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in categorical:\n",
    "    df_reduced = replace_na_categorical(df_reduced,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(df_reduced,columns=categorical)\n",
    "dummy.to_csv('data/df_cleaned.csv',index=False)\n",
    "dummy_sampled = dummy.sample(frac=0.1,random_state=2018).reset_index(drop=True)\n",
    "dummy_sampled.to_csv('data/df_sampled_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set up the model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_HPS_search(train_X,train_Y,model,n_iter,score,cv,weights=None):\n",
    "    \n",
    "    model_selection, params = get_model_and_params(model)\n",
    "    \n",
    "    RS = RandomizedSearchCV(model_selection,param_distributions=params,n_iter=n_iter,scoring=score,cv=cv,error_score='0')\n",
    "    RS.fit(train_X,train_Y,sample_weight=weights)\n",
    "    print(\"Best params - \", RS.best_params_)\n",
    "    print(\"Highest %s = %s\"%(score,RS.best_score_))\n",
    "    \n",
    "    return RS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_and_params(model):\n",
    "    \n",
    "    model_selection = {\n",
    "        'RFC':RandomForestClassifier(),\n",
    "        'GBC':GradientBoostingClassifier(),\n",
    "        'LR':LogisticRegression()\n",
    "    }\n",
    "    \n",
    "    model_hyperparameters = {\n",
    "        'RFC':{\n",
    "            'n_estimators':range(1,101),\n",
    "            'max_depth':range(1,50),\n",
    "            'n_jobs':[-1],\n",
    "            'criterion':['gini','entropy'],\n",
    "            'class_weight':['balanced_subsample']\n",
    "        },\n",
    "        'GBC':{\n",
    "            'loss':['deviance','exponential'],\n",
    "            'learning_rate':10**np.linspace(-4,-1,10),\n",
    "            'n_estimators':range(50,150)\n",
    "        },\n",
    "        'LR':{\n",
    "            'C':10.**np.linspace(-3,3,20),\n",
    "            'tol':10**np.linspace(-5,-1,20),\n",
    "            'penalty':['l2'],\n",
    "            'class_weight':['balanced']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return model_selection[model], model_hyperparameters[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_clf(model,train_X,train_Y,params,weights=None):\n",
    "    \n",
    "    clf = get_model_and_params(model)[0]\n",
    "    \n",
    "    clf.set_params(**params)\n",
    "    clf.fit(train_X,train_Y,sample_weight=weights)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_error_and_discrimination(clf,test_X,test_Y,df_test,sensitive_features):\n",
    "\n",
    "    Y_predict = clf.predict(test_X)\n",
    "\n",
    "    print(\"F1 score = %s\" %f1_score(test_Y,Y_predict))\n",
    "    print(\"Precision score = %s\" %precision_score(test_Y,Y_predict))\n",
    "    print(\"Recall score = %s\" %recall_score(test_Y,Y_predict))\n",
    "    print(\"Accuracy score = %s\" %accuracy_score(test_Y,Y_predict))\n",
    "\n",
    "    print()\n",
    "\n",
    "    s = np.asarray(df_test[sensitive_features])\n",
    "    print(\"Discrimination_ratio = %s\\nDiscrimination_normalized = %s\" %(discrimination(np.expand_dims(Y_predict,1),s)))\n",
    "\n",
    "    print()\n",
    "\n",
    "    features = ['driver_race_Asian','driver_race_Black','driver_race_White','driver_race_Hispanic']\n",
    "    \n",
    "    bias = [\n",
    "    features,\n",
    "    list(df_test[features].mean()),\n",
    "    list(df_test[features].loc[Y_predict==1].mean()),\n",
    "    list(df_test[features].loc[Y_predict==0].mean())\n",
    "    ]\n",
    "        \n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrimination(y,s):\n",
    "    \n",
    "    disc_ratio = float(len(s[s==0]))*float(y[s==1].sum())/(float(y[s==0].sum())*float(len(s[s==1])))\n",
    "    disc_norm = np.abs(float(y[s==0].sum())/float(len(s[s==0])) - float(y[s==1].sum())/float(len(s[s==1])))/float(y.mean())\n",
    "    \n",
    "    return disc_ratio, disc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_df(df,f_balance):\n",
    "    \n",
    "    if (f_balance == -1):\n",
    "        return df\n",
    "    \n",
    "    cond = df['is_arrested']==True\n",
    "\n",
    "    df_balanced = pd.concat(\n",
    "        [df.loc[cond],\n",
    "         df.loc[~cond].sample(n=df.loc[cond].shape[0]*f_balance)],\n",
    "        axis=0\n",
    "    ).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/df_sampled_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feature = 'is_arrested'\n",
    "sensitive_features = ['driver_race_Black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray(df_train.drop([target_feature],axis=1))\n",
    "train_Y = np.asarray(df_train[target_feature]).astype(int)\n",
    "\n",
    "test_X = np.asarray(df_test.drop([target_feature],axis=1))\n",
    "test_Y = np.asarray(df_test[target_feature]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.228466707392\n",
      "Precision score = 0.52791718946\n",
      "Recall score = 0.145777392811\n",
      "Accuracy score = 0.993855011353\n",
      "\n",
      "Discrimination ratio = 2.653648918290935\n",
      "Discrimination (normalized) = 1.37081408312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'n_jobs': -1, 'n_estimators': 47, 'max_depth': 8, 'criterion': 'gini', 'class_weight': None}\n",
    "weights = np.ones(train_Y.shape[0])\n",
    "weights[train_Y==1] = 1.\n",
    "weights[train_Y==0] = 0.5\n",
    "RFC = run_clf('RFC',train_X,train_Y,params,weights=weights)\n",
    "bias = check_error_and_discrimination(RFC,test_X,test_Y,df_test,sensitive_features)\n",
    "df_bias = pd.DataFrame(bias).transpose()\n",
    "df_bias.columns = [\"Group\",\"N_group/N_total\",\"N_group/N_total|(predicted arrest)\",\"N_group/N_total|(predicted not arrest)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>N_group/N_total</th>\n",
       "      <th>N_group/N_total|(predicted arrest)</th>\n",
       "      <th>N_group/N_total|(predicted not arrest)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>driver_race_Asian</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>0.00313676</td>\n",
       "      <td>0.013573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>driver_race_Black</td>\n",
       "      <td>0.12477</td>\n",
       "      <td>0.274467</td>\n",
       "      <td>0.124512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>driver_race_White</td>\n",
       "      <td>0.83926</td>\n",
       "      <td>0.696989</td>\n",
       "      <td>0.839506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>driver_race_Hispanic</td>\n",
       "      <td>0.0217688</td>\n",
       "      <td>0.0244668</td>\n",
       "      <td>0.0217642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Group N_group/N_total N_group/N_total|(predicted arrest)  \\\n",
       "0     driver_race_Asian        0.013555                         0.00313676   \n",
       "1     driver_race_Black         0.12477                           0.274467   \n",
       "2     driver_race_White         0.83926                           0.696989   \n",
       "3  driver_race_Hispanic       0.0217688                          0.0244668   \n",
       "\n",
       "  N_group/N_total|(predicted not arrest)  \n",
       "0                               0.013573  \n",
       "1                               0.124512  \n",
       "2                               0.839506  \n",
       "3                              0.0217642  "
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin setting up the VFAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights_biases(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=tf.sqrt(0.5 / float(shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activate(x,activation):\n",
    "    if (activation == 'relu'):\n",
    "        return tf.nn.relu(x)\n",
    "    elif (activation == 'sigmoid'):\n",
    "        return tf.nn.sigmoid(x)\n",
    "    elif (activation == 'tanh'):\n",
    "        return tf.nn.tanh(x)\n",
    "    elif (activation == 'softmax'):\n",
    "        return tf.nn.softmax(x)\n",
    "    elif (activation == 'linear'):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(list_arrays,batch_size,index_shuffled,b):\n",
    "    \n",
    "    return [x[index_shuffled[b*batch_size:(b+1)*batch_size]] for x in list_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(dims,N_epochs=1000,print_freq=100,batch_size=100,lr=1e-3,alpha=1.,beta=0.):\n",
    "\n",
    "    params = {\n",
    "        'enc1':{\n",
    "            'in_dim':dims['x']+dims['s'],\n",
    "            'hid_dim':dims['enc1_hid'],\n",
    "            'out_dim':dims['z1'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'linear',\n",
    "                'log_sigma':'linear'\n",
    "            }\n",
    "        },   \n",
    "        'enc2':{\n",
    "            'in_dim':dims['z1']+1,\n",
    "            'hid_dim':dims['enc2_hid'],\n",
    "            'out_dim':dims['z2'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'linear',\n",
    "                'log_sigma':'linear'\n",
    "            }\n",
    "        },\n",
    "        'dec1':{\n",
    "            'in_dim':dims['z2']+1,\n",
    "            'hid_dim':dims['dec1_hid'],\n",
    "            'out_dim':dims['z1'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'linear',\n",
    "                'log_sigma':'linear'\n",
    "            }\n",
    "        },\n",
    "        'dec2':{\n",
    "            'in_dim':dims['z1']+dims['s'],\n",
    "            'hid_dim':dims['dec2_hid'],\n",
    "            'out_dim':dims['x']+dims['s'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'sigmoid',\n",
    "                'log_sigma':'sigmoid'\n",
    "            }\n",
    "        },\n",
    "        'us':{\n",
    "            'in_dim':dims['z1'],\n",
    "            'hid_dim':dims['us_hid'],\n",
    "            'out_dim':dims['y_cat'],\n",
    "            'act':{\n",
    "                'hid':'relu',\n",
    "                'mu':'softmax',\n",
    "                'log_sigma':'softmax'\n",
    "            }\n",
    "        },\n",
    "        'N_epochs':N_epochs,\n",
    "        'print_frequency':print_freq,\n",
    "        'batch_size':batch_size,\n",
    "        'lr':lr,\n",
    "        'alpha':alpha,\n",
    "        'beta':beta\n",
    "    }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights_biases(params):\n",
    "\n",
    "    weights = {\n",
    "        'enc1':{\n",
    "            'hid':gen_weights_biases([params['enc1']['in_dim'],params['enc1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc1']['hid_dim'],params['enc1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc1']['hid_dim'],params['enc1']['out_dim']])\n",
    "        },\n",
    "        'enc2':{\n",
    "            'hid':gen_weights_biases([params['enc2']['in_dim'],params['enc2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc2']['hid_dim'],params['enc2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc2']['hid_dim'],params['enc2']['out_dim']])\n",
    "        },\n",
    "        'dec1':{\n",
    "            'hid':gen_weights_biases([params['dec1']['in_dim'],params['dec1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec1']['hid_dim'],params['dec1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec1']['hid_dim'],params['dec1']['out_dim']])\n",
    "        },\n",
    "        'dec2':{\n",
    "            'hid':gen_weights_biases([params['dec2']['in_dim'],params['dec2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec2']['hid_dim'],params['dec2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec2']['hid_dim'],params['dec2']['out_dim']])\n",
    "        },\n",
    "        'us':{\n",
    "            'hid':gen_weights_biases([params['us']['in_dim'],params['us']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['us']['hid_dim'],params['us']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['us']['hid_dim'],params['us']['out_dim']])\n",
    "        }       \n",
    "    }\n",
    "\n",
    "    bias = {\n",
    "        'enc1':{\n",
    "            'hid':gen_weights_biases([params['enc1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc1']['out_dim']])\n",
    "        },\n",
    "        'enc2':{\n",
    "            'hid':gen_weights_biases([params['enc2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['enc2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['enc2']['out_dim']])\n",
    "        },\n",
    "        'dec1':{\n",
    "            'hid':gen_weights_biases([params['dec1']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec1']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec1']['out_dim']])\n",
    "        },\n",
    "        'dec2':{\n",
    "            'hid':gen_weights_biases([params['dec2']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['dec2']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['dec2']['out_dim']])\n",
    "        },\n",
    "        'us':{\n",
    "            'hid':gen_weights_biases([params['us']['hid_dim']]),\n",
    "            'mu':gen_weights_biases([params['us']['out_dim']]),\n",
    "            'log_sigma':gen_weights_biases([params['us']['out_dim']])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x_in,weights,bias,activation,epsilon):\n",
    "    \n",
    "    hidden_en = activate(tf.matmul(x_in,weights['hid'])+bias['hid'],activation['hid'])\n",
    "\n",
    "    mu = activate(tf.matmul(hidden_en,weights['mu'])+bias['mu'],activation['mu'])\n",
    "\n",
    "    log_sigma = activate(tf.matmul(hidden_en,weights['log_sigma'])+bias['log_sigma'],activation['log_sigma'])\n",
    "\n",
    "    return mu + tf.exp(log_sigma / 2) * epsilon, mu, log_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KL(mu1,log_sigma_sq1,mu2=0.,log_sigma_sq2=0.):\n",
    "    return 0.5*tf.reduce_sum(log_sigma_sq2-log_sigma_sq1-1+(tf.exp(log_sigma_sq1)+tf.pow(mu1-mu2,2))/tf.exp(log_sigma_sq2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LH(x,mu,log_sigma):\n",
    "    return 0.5 * tf.reduce_sum(np.log(2 * np.pi) + log_sigma + tf.pow(x - mu,2) / tf.exp(log_sigma), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_VFAE(train_X,train_Y,train_s,test_X,test_Y,test_s,weights,bias,params,dims):\n",
    "\n",
    "    x = tf.placeholder(tf.float32,shape=[None,dims['x']],name='x')\n",
    "    s = tf.placeholder(tf.float32,shape=[None,dims['s']],name='s')\n",
    "    y = tf.placeholder(tf.float32,shape=[None,1],name='y')\n",
    "    index = tf.placeholder(tf.int32,shape=[None,1],name='index')\n",
    "    \n",
    "    epsilon0 = tf.random_normal([params['enc1']['out_dim']], dtype=tf.float32, name='epsilon0')\n",
    "    z1_enc, z1_enc_mu, z1_enc_log_sigma = MLP(tf.concat([x,s],axis=1),weights['enc1'],bias['enc1'],params['enc1']['act'],epsilon0)\n",
    "    \n",
    "    epsilon1 = tf.random_normal([params['enc2']['out_dim']], dtype=tf.float32, name='epsilon1')\n",
    "    z2_enc, z2_enc_mu, z2_enc_log_sigma = MLP(tf.concat([z1_enc,y],axis=1),weights['enc2'],bias['enc2'],params['enc2']['act'],epsilon1)\n",
    "    \n",
    "    epsilon2 = tf.random_normal([params['dec1']['out_dim']], dtype=tf.float32, name='epsilon2')\n",
    "    z1_dec, z1_dec_mu, z1_dec_log_sigma = MLP(tf.concat([z2_enc,y],axis=1),weights['dec1'],bias['dec1'],params['dec1']['act'],epsilon2)\n",
    "    \n",
    "    epsilon3 = tf.zeros([params['dec2']['out_dim']], dtype=tf.float32, name='epsilon3')\n",
    "    x_out = MLP(tf.concat([z1_dec,s],axis=1),weights['dec2'],bias['dec2'],params['dec2']['act'],epsilon3)[0]\n",
    "\n",
    "    epsilon4 = tf.zeros([params['us']['out_dim']], dtype=tf.float32, name='epsilon4')\n",
    "    y_us = MLP(z1_enc,weights['us'],bias['us'],params['us']['act'],epsilon4)[0]\n",
    "    \n",
    "    KL_z1 = KL(z1_enc_mu,z1_enc_log_sigma,z1_dec_mu,z1_dec_log_sigma)\n",
    "    KL_z2 = KL(z2_enc_mu,z2_enc_log_sigma)\n",
    "    \n",
    "    LH_x = tf.reduce_sum(tf.concat([x,s],axis=1) * tf.log(1e-10+x_out) + (1 - tf.concat([x,s],axis=1)) * tf.log(1e-10+1 - x_out),axis=1)\n",
    "    \n",
    "    idx = tf.stack([index, tf.cast(y,tf.int32)], axis=-1)\n",
    "\n",
    "    LH_y = tf.reduce_sum(tf.log(1e-10+tf.gather_nd(y_us, idx)),axis=1)\n",
    "\n",
    "    MMD = 0.\n",
    "    \n",
    "    loss = -(-tf.reduce_mean(KL_z1)-tf.reduce_mean(KL_z2)+tf.reduce_mean(LH_x)-params['alpha']*tf.reduce_mean(LH_y))# - beta*MMD\n",
    "    -tf.reduce_mean(-KL_z1-KL_z2+LH_x-params['alpha']*LH_y)# - params['beta']*MMD\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['lr'])\n",
    "\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    index_shuffled = np.arange(train_X.shape[0])\n",
    "    np.random.shuffle(index_shuffled)\n",
    "\n",
    "    N_batches = int(float(train_X.shape[0])/float(params['batch_size']))\n",
    "    \n",
    "    for i in range(params['N_epochs']):\n",
    "        \n",
    "        for b in range(N_batches):\n",
    "            \n",
    "            batch_X, batch_Y, batch_s = get_batch([train_X,train_Y,train_s],params['batch_size'],index_shuffled,b)\n",
    "\n",
    "            batch_dict = {x:batch_X,y:batch_Y,s:batch_s,index:np.arange(batch_Y.shape[0]).reshape(batch_Y.shape[0],1)}\n",
    "            train_dict = {x:train_X,y:train_Y,s:train_s,index:np.arange(train_Y.shape[0]).reshape(train_Y.shape[0],1)}\n",
    "\n",
    "            sess.run(train,feed_dict=batch_dict)\n",
    "\n",
    "            if ((i % params['print_frequency'] == 0) and (b == N_batches-1)):\n",
    "\n",
    "                print(\"Epoch %s: batch loss = %s and global loss = %s\"%(i,\n",
    "                        sess.run(loss,feed_dict=batch_dict),\n",
    "                        sess.run(loss,feed_dict=train_dict)))\n",
    "                print(sess.run([tf.reduce_mean(KL_z1),tf.reduce_mean(KL_z2),tf.reduce_mean(LH_x),tf.reduce_mean(LH_y)],feed_dict=batch_dict))\n",
    "                test_dict = {x:test_X,y:test_Y,s:test_s,index:np.arange(test_Y.shape[0]).reshape(test_Y.shape[0],1)}\n",
    "                np.save('output/latent_x_epoch_%s'%(i),sess.run(z1_enc,feed_dict=test_dict))\n",
    "            \n",
    "    test_dict = {x:test_X,y:test_Y,s:test_s,index:np.arange(test_Y.shape[0]).reshape(test_Y.shape[0],1)}\n",
    "    return sess.run([x_out,z1_enc,loss],feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into a training and cross-validation sets, to determine the optimal hyperparameters for the VFAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_X_Y_s(df,target_feature,sensitive_features):\n",
    "    \n",
    "    X = np.asarray(df.drop([target_feature]+sensitive_features,axis=1))\n",
    "    Y = np.expand_dims(np.asarray(df[target_feature]).astype(int),1)\n",
    "    s = np.expand_dims(np.asarray(df[sensitive_features]).astype(int),1)\n",
    "\n",
    "    return X, Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_feature = 'is_arrested'\n",
    "sensitive_features = ['driver_race_Black']\n",
    "\n",
    "df_VFAE_train, df_VFAE_CV = train_test_split(df,train_size=0.33)\n",
    "df_VFAE_CV, df_VFAE_test = train_test_split(df_VFAE_CV,test_size=0.5)\n",
    "\n",
    "df_VFAE_train = df_VFAE_train.reset_index(drop=True)\n",
    "df_VFAE_CV = df_VFAE_CV.reset_index(drop=True)\n",
    "df_VFAE_test = df_VFAE_test.reset_index(drop=True)\n",
    "\n",
    "VFAE_train_X, VFAE_train_Y, VFAE_train_s = obtain_X_Y_s(df_VFAE_train,target_feature,sensitive_features)\n",
    "VFAE_CV_X, VFAE_CV_Y, VFAE_CV_s = obtain_X_Y_s(df_VFAE_CV,target_feature,sensitive_features)\n",
    "VFAE_test_X, VFAE_test_Y, VFAE_test_s = obtain_X_Y_s(df_VFAE_test,target_feature,sensitive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin analysis for dims = [50, 50, 100, 100, 100, 100, 100], lr = 0.001, and alpha = 0.1:\n",
      "Epoch 0: batch loss = 66.4549 and global loss = 64.8756\n",
      "[6.3467026, 2.9205317, -55.297737, -1.4578407]\n",
      "Epoch 5: batch loss = 16.6861 and global loss = 16.3218\n",
      "[1.1768936, 0.52575457, -16.81337, -22.89934]\n",
      "Epoch 10: batch loss = 14.9242 and global loss = 15.4115\n",
      "[1.1704465, 0.21019964, -16.432467, -22.899651]\n",
      "Epoch 15: batch loss = 15.2118 and global loss = 15.047\n",
      "[0.86113799, 0.37193912, -16.182674, -22.899651]\n",
      "Epoch 20: batch loss = 16.262 and global loss = 14.3666\n",
      "[0.23268989, 0.059066754, -16.054325, -22.899651]\n",
      "Epoch 25: batch loss = 13.8851 and global loss = 13.8839\n",
      "[0.27517316, 0.022389447, -16.078943, -22.899651]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-707-fcc7a8ad998f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_weights_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0menc_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_VFAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVFAE_train_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVFAE_train_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVFAE_train_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVFAE_CV_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVFAE_CV_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVFAE_CV_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The CV loss = %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hidden units in the hidden layers\n",
    "# [z1, z2, encoding1, encoding2, decoding1, decoding2, unsupervised]\n",
    "hidden_dim_list = [\n",
    "    [50,50,100,100,100,100,100]\n",
    "]\n",
    "\n",
    "lr_list = [1e-3]\n",
    "\n",
    "alpha_list = [1e-1]\n",
    "\n",
    "loss_min = 1e3\n",
    "\n",
    "for d in hidden_dim_list:\n",
    "    \n",
    "    for lr in lr_list:\n",
    "        \n",
    "        for alpha in alpha_list:\n",
    "        \n",
    "            print(\"Begin analysis for dims = %s, lr = %s, and alpha = %s:\"%(d,lr,alpha))\n",
    "\n",
    "            dims = {\n",
    "                'x':VFAE_train_X.shape[1],\n",
    "                'y_cat':len(np.unique(VFAE_train_Y)),\n",
    "                's':VFAE_train_s.shape[1],\n",
    "                'z1':d[0],\n",
    "                'z2':d[1],\n",
    "                'enc1_hid':d[2],\n",
    "                'enc2_hid':d[3],\n",
    "                'dec1_hid':d[4],\n",
    "                'dec2_hid':d[5],\n",
    "                'us_hid':d[6]\n",
    "            }\n",
    "\n",
    "            params = initialize_params(dims,N_epochs=30,lr=lr,print_freq=5,batch_size=10000,alpha=alpha)\n",
    "            weights, bias = initialize_weights_biases(params)\n",
    "            enc_X, latent_X, loss = train_VFAE(VFAE_train_X,VFAE_train_Y,VFAE_train_s,VFAE_CV_X,VFAE_CV_Y,VFAE_CV_s,weights,bias,params,dims)\n",
    "\n",
    "            print(\"The CV loss = %s\"%(loss))\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "            if (loss < loss_min):\n",
    "                loss_min = loss\n",
    "                d_optimal = d\n",
    "                lr_optimal = lr\n",
    "                alpha_optimal = alpha\n",
    "                latent_X_optimal = latent_X\n",
    "                enc_X_optimal = enc_X\n",
    "            \n",
    "print(\"The minimum loss of %s was obtained using dims = %s, lr = %s, and alpha = %s\"%(loss_min,d_optimal,lr_optimal,alpha_optimal))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run with the optimal params\n",
    "dims = {\n",
    "    'x':VFAE_test_X.shape[1],\n",
    "    'y_cat':len(np.unique(VFAE_test_Y)),\n",
    "    's':VFAE_test_s.shape[1],\n",
    "    'z1':d_optimal[0],\n",
    "    'z2':d_optimal[1],\n",
    "    'enc1_hid':d_optimal[2],\n",
    "    'enc2_hid':d_optimal[3],\n",
    "    'dec1_hid':d_optimal[4],\n",
    "    'dec2_hid':d_optimal[5],\n",
    "    'us_hid':d_optimal[6]\n",
    "}\n",
    "\n",
    "params = initialize_params(dims,N_epochs=40,lr=lr_optimal,print_freq=10,batch_size=1000,alpha=alpha_optimal)\n",
    "weights, bias = initialize_weights_biases(params)\n",
    "enc_X, latent_X, loss, y_us, weights_post, bias_post = train_VFAE(VFAE_test_X,VFAE_test_Y,VFAE_test_s,VFAE_test_X,VFAE_test_Y,VFAE_test_s,weights,bias,params,dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(np.arange(VFAE_CV_X.shape[0]),test_size=0.3)\n",
    "RF_train_X, RF_test_X = VFAE_CV_X[train_index], VFAE_CV_X[test_index]\n",
    "RF_train_Y, RF_test_Y = VFAE_CV_Y[train_index].ravel(), VFAE_CV_Y[test_index].ravel()\n",
    "RF_train_s, RF_test_s = VFAE_CV_s[train_index], VFAE_CV_s[test_index]\n",
    "df_train, df_test = df_VFAE_CV.loc[train_index], df_VFAE_CV.loc[test_index]\n",
    "\n",
    "enc_train_X, enc_test_X = latent_X[train_index], latent_X[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.309278350515\n",
      "Precision score = 0.590551181102\n",
      "Recall score = 0.209497206704\n",
      "Accuracy score = 0.993963311349\n",
      "\n",
      "Discrimination ratio = 1.956154987824032\n",
      "Discrimination (normalized) = 0.853114563411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'n_jobs': -1, 'n_estimators': 47, 'max_depth': 8, 'criterion': 'gini', 'class_weight': None}\n",
    "weights = np.ones(RF_train_Y.shape[0])\n",
    "weights[RF_train_Y==1] = 1.\n",
    "weights[RF_train_Y==0] = 0.5\n",
    "RFC = run_clf('RFC',enc_train_X,RF_train_Y,params,weights=weights)\n",
    "bias = check_error_and_discrimination(RFC,enc_test_X,RF_test_Y,df_test,sensitive_features)\n",
    "df_bias = pd.DataFrame(bias).transpose()\n",
    "df_bias.columns = [\"Group\",\"N_group/N_total\",\"N_group/N_total|(predicted arrest)\",\"N_group/N_total|(predicted not arrest)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>N_group/N_total</th>\n",
       "      <th>N_group/N_total|(predicted arrest)</th>\n",
       "      <th>N_group/N_total|(predicted not arrest)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>driver_race_Asian</td>\n",
       "      <td>0.0125599</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0125887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>driver_race_Black</td>\n",
       "      <td>0.12632</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>0.126104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>driver_race_White</td>\n",
       "      <td>0.839262</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.839399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>driver_race_Hispanic</td>\n",
       "      <td>0.0211014</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0211498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Group N_group/N_total N_group/N_total|(predicted arrest)  \\\n",
       "0     driver_race_Asian       0.0125599                                  0   \n",
       "1     driver_race_Black         0.12632                           0.220472   \n",
       "2     driver_race_White        0.839262                           0.779528   \n",
       "3  driver_race_Hispanic       0.0211014                                  0   \n",
       "\n",
       "  N_group/N_total|(predicted not arrest)  \n",
       "0                              0.0125887  \n",
       "1                               0.126104  \n",
       "2                               0.839399  \n",
       "3                              0.0211498  "
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
